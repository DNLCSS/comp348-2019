{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W05L1-3-Tagging.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "AWyCmzO622th",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building a Part of Speech Tagger with Keras\n",
        "\n",
        "This notebook is based on nlpforhackers' post: https://nlpforhackers.io/lstm-pos-tagger-keras/. We will look at how to build a part of speech tagger using a LSTM layer.\n",
        "\n",
        "We will use NLTK's treebank corpus. This corpus has, among other kinds of information, annotations about the parts of speech of the words."
      ]
    },
    {
      "metadata": {
        "id": "dugp1ZSX22tm",
        "colab_type": "code",
        "outputId": "9100f3ec-841b-4feb-cc25-fad4239b0478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        " \n",
        "print(tagged_sentences[0])\n",
        "print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "Tagged sentences:  3914\n",
            "Tagged words: 100676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "aKG3Wi1k22tx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As always, before training a model, we need to split the data in training and testing data:"
      ]
    },
    {
      "metadata": {
        "id": "B0BTKbIe22tz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(1234)\n",
        "tagged_sentences = list(tagged_sentences) # we convert the data to a list\n",
        "random.shuffle(tagged_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sEkj81QG22t4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "threshold = int(len(tagged_sentences)*.6)\n",
        "train = tagged_sentences[:threshold]\n",
        "test = tagged_sentences[threshold:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "42eTQ3kQ22t9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let’s restructure the data a bit. Let’s separate the words from the tags."
      ]
    },
    {
      "metadata": {
        "id": "J-DZLM5e22t_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        " \n",
        "train_sentences, train_tags =[], [] \n",
        "for tagged_sentence in train:\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    train_sentences.append(np.array(sentence))\n",
        "    train_tags.append(np.array(tags))\n",
        "    \n",
        "test_sentences, test_tags =[], [] \n",
        "for tagged_sentence in test:\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    test_sentences.append(np.array(sentence))\n",
        "    test_tags.append(np.array(tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aKlHJ21O22uE",
        "colab_type": "code",
        "outputId": "9ae85f10-7b45-4096-99cd-8ef06861ba83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_sentences[5])\n",
        "print(train_tags[5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['They' 'are' 'keeping' 'a' 'close' 'watch' 'on' 'the' 'yield' 'on' 'the'\n",
            " 'S&P' '500' '.']\n",
            "['PRP' 'VBP' 'VBG' 'DT' 'JJ' 'NN' 'IN' 'DT' 'NN' 'IN' 'DT' 'NNP' 'CD' '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "TO6Fo_ez22uK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now convert the words and tags to indices. We will reserve index 0 for padding, and index 1 for words that are out of the vocabulary (**OOV - Out of Vocabulary**)."
      ]
    },
    {
      "metadata": {
        "id": "Dpy45jUr22uM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words, tags = set([]), set([])\n",
        " \n",
        "for s in train_sentences:\n",
        "    for w in s:\n",
        "        words.add(w.lower())\n",
        " \n",
        "for ts in train_tags:\n",
        "    for t in ts:\n",
        "        tags.add(t)\n",
        " \n",
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # The special value used for padding\n",
        "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
        " \n",
        "tag2index = {t: i + 2 for i, t in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0  # The special value used to padding\n",
        "tag2index['-OOV-'] = 1  # There may also be unknown tags in the test data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "tz7jJt1R22uQ",
        "colab_type": "code",
        "outputId": "c1aef435-600b-4805-a949-9f5e72f1becc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        " \n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    s_int = []\n",
        "    for t in s:\n",
        "        try:\n",
        "            s_int.append(tag2index[t])\n",
        "        except KeyError:\n",
        "            s_int.append(tag2index['-OOV-'])\n",
        "            \n",
        "    test_tags_y.append(s_int)\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4303, 389, 7149, 3356, 4538, 1088, 6477, 6831]\n",
            "[4842, 5670, 2706, 5799, 3360, 1, 7364, 7238, 3651, 162, 4296, 7238, 1, 5767, 3180, 4538, 3741, 6555, 7238, 5256, 8507, 1, 6831]\n",
            "[7, 45, 29, 34, 38, 19, 19, 16]\n",
            "[33, 34, 30, 28, 34, 30, 14, 18, 10, 33, 30, 18, 30, 14, 30, 38, 20, 30, 18, 14, 43, 30, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "tyBmXkCw22uW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since Keras can only deal with fixed size sequences, we will pad all sequences to fit the longest sentence in the training data."
      ]
    },
    {
      "metadata": {
        "id": "e0ZdPDZA22uY",
        "colab_type": "code",
        "outputId": "735a8b93-98cd-40ff-cddb-7a0aa6f742c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(MAX_LENGTH)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "S8fOCFoD22ud",
        "colab_type": "code",
        "outputId": "c91f0a07-ec7a-4fb7-c4cb-6700d1f21aa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1187
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[4303  389 7149 3356 4538 1088 6477 6831    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[4842 5670 2706 5799 3360    1 7364 7238 3651  162 4296 7238    1 5767\n",
            " 3180 4538 3741 6555 7238 5256 8507    1 6831    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[ 7 45 29 34 38 19 19 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n",
            "[33 34 30 28 34 30 14 18 10 33 30 18 30 14 30 38 20 30 18 14 43 30 16  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "f0w8DzeQ22uj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now define the model. \n",
        "A simple approach to build a part-of-speech tagger in Keras is to stack a dense layer at the output of each RNN cell. This is achieved with `TimeDistributed`. Note  how the output of the model is now a list of sequences of vectors. Each vector will contain the probability of each tag. In particular:\n",
        "\n",
        "* We will introduce an embedding layer.\n",
        "* THe LSTM layer will have the parameter `return_sequences=True` so that we have access to the entire sequence.\n",
        "* The output of each LSTM cell will have a `Dense` layer. We can do this using the `TimeDistributed` layer."
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "f6buON2e22ul",
        "colab_type": "code",
        "outputId": "734aff66-2695-4a76-b2de-a7c5fa4a6040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 271, 128)          1111424   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 271, 256)          394240    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 271, 47)           12079     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 1,517,743\n",
            "Trainable params: 1,517,743\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "UeE6WfGH22ur",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We also need to convert the labels into one-hot encoding. Since the input are sequences of labels we cannot use Keras' `to_categorical` so we define our own function:"
      ]
    },
    {
      "metadata": {
        "id": "bJTyy7hA22uu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xWr3YL_S22uy",
        "colab_type": "code",
        "outputId": "78fd46b4-8ab1-4853-f8fd-9133c343c797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "print(cat_train_tags_y[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "XLqIilKl22u5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can finally train the model:"
      ]
    },
    {
      "metadata": {
        "id": "7pTh_pR922vA",
        "colab_type": "code",
        "outputId": "ac27ebf5-9172-469c-9ff2-34817c3c0a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1583
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_split=0.2)\n",
        " "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 1878 samples, validate on 470 samples\n",
            "Epoch 1/40\n",
            "1878/1878 [==============================] - 12s 6ms/step - loss: 1.7439 - acc: 0.8436 - val_loss: 0.6010 - val_acc: 0.9036\n",
            "Epoch 2/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.6316 - acc: 0.8845 - val_loss: 0.4542 - val_acc: 0.9037\n",
            "Epoch 3/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.4090 - acc: 0.9056 - val_loss: 0.3928 - val_acc: 0.9038\n",
            "Epoch 4/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3843 - acc: 0.9058 - val_loss: 0.3834 - val_acc: 0.9039\n",
            "Epoch 5/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3704 - acc: 0.9060 - val_loss: 0.3746 - val_acc: 0.9041\n",
            "Epoch 6/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3600 - acc: 0.9062 - val_loss: 0.3602 - val_acc: 0.9041\n",
            "Epoch 7/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3449 - acc: 0.9066 - val_loss: 0.3520 - val_acc: 0.9050\n",
            "Epoch 8/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3357 - acc: 0.9080 - val_loss: 0.3454 - val_acc: 0.9060\n",
            "Epoch 9/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3314 - acc: 0.9097 - val_loss: 0.3361 - val_acc: 0.9096\n",
            "Epoch 10/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3230 - acc: 0.9109 - val_loss: 0.3395 - val_acc: 0.9096\n",
            "Epoch 11/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3168 - acc: 0.9132 - val_loss: 0.3370 - val_acc: 0.9120\n",
            "Epoch 12/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3112 - acc: 0.9146 - val_loss: 0.3433 - val_acc: 0.9128\n",
            "Epoch 13/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3069 - acc: 0.9158 - val_loss: 0.3557 - val_acc: 0.9134\n",
            "Epoch 14/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3019 - acc: 0.9170 - val_loss: 0.3368 - val_acc: 0.9157\n",
            "Epoch 15/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2989 - acc: 0.9184 - val_loss: 0.3295 - val_acc: 0.9166\n",
            "Epoch 16/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2936 - acc: 0.9194 - val_loss: 0.3215 - val_acc: 0.9175\n",
            "Epoch 17/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2899 - acc: 0.9200 - val_loss: 0.3315 - val_acc: 0.9174\n",
            "Epoch 18/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2875 - acc: 0.9203 - val_loss: 0.3321 - val_acc: 0.9173\n",
            "Epoch 19/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2852 - acc: 0.9206 - val_loss: 0.3030 - val_acc: 0.9190\n",
            "Epoch 20/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2826 - acc: 0.9208 - val_loss: 0.2933 - val_acc: 0.9196\n",
            "Epoch 21/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2806 - acc: 0.9209 - val_loss: 0.2909 - val_acc: 0.9198\n",
            "Epoch 22/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2778 - acc: 0.9211 - val_loss: 0.2937 - val_acc: 0.9196\n",
            "Epoch 23/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2752 - acc: 0.9219 - val_loss: 0.2893 - val_acc: 0.9210\n",
            "Epoch 24/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2726 - acc: 0.9233 - val_loss: 0.2879 - val_acc: 0.9212\n",
            "Epoch 25/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2697 - acc: 0.9242 - val_loss: 0.2878 - val_acc: 0.9219\n",
            "Epoch 26/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2661 - acc: 0.9253 - val_loss: 0.2826 - val_acc: 0.9263\n",
            "Epoch 27/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2619 - acc: 0.9288 - val_loss: 0.2790 - val_acc: 0.9283\n",
            "Epoch 28/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2568 - acc: 0.9324 - val_loss: 0.2738 - val_acc: 0.9311\n",
            "Epoch 29/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2499 - acc: 0.9357 - val_loss: 0.2657 - val_acc: 0.9343\n",
            "Epoch 30/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2413 - acc: 0.9392 - val_loss: 0.2576 - val_acc: 0.9370\n",
            "Epoch 31/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2316 - acc: 0.9418 - val_loss: 0.2468 - val_acc: 0.9385\n",
            "Epoch 32/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2206 - acc: 0.9445 - val_loss: 0.2317 - val_acc: 0.9440\n",
            "Epoch 33/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2088 - acc: 0.9477 - val_loss: 0.2192 - val_acc: 0.9463\n",
            "Epoch 34/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1969 - acc: 0.9510 - val_loss: 0.2078 - val_acc: 0.9486\n",
            "Epoch 35/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1844 - acc: 0.9544 - val_loss: 0.1961 - val_acc: 0.9523\n",
            "Epoch 36/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1722 - acc: 0.9577 - val_loss: 0.1874 - val_acc: 0.9539\n",
            "Epoch 37/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1601 - acc: 0.9609 - val_loss: 0.1740 - val_acc: 0.9585\n",
            "Epoch 38/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1489 - acc: 0.9641 - val_loss: 0.1717 - val_acc: 0.9572\n",
            "Epoch 39/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1384 - acc: 0.9671 - val_loss: 0.1551 - val_acc: 0.9629\n",
            "Epoch 40/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1275 - acc: 0.9704 - val_loss: 0.1448 - val_acc: 0.9662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "5zNVFn7W22vM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the results look too good, it's because they are too good! We need to ignore the classification of the padded symbols, but let's leave it aside for now. Below is an evaluation using the test data."
      ]
    },
    {
      "metadata": {
        "id": "8YVnb9mv22vP",
        "colab_type": "code",
        "outputId": "68d0e3d2-6464-4f23-e4e8-7d171703a4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
        "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1566/1566 [==============================] - 11s 7ms/step\n",
            "acc: 96.64550643618841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "h197tLfl22vY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's find the PoS of two test sentences:"
      ]
    },
    {
      "metadata": {
        "id": "TJn9HsNN22va",
        "colab_type": "code",
        "outputId": "6d458fae-6abd-44bb-abbf-f00a034475c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "test_samples = [\n",
        "    \"running is very important for me .\".split(),\n",
        "    \"I was running every day for a month .\".split()\n",
        "]\n",
        "print(test_samples)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "NPD1nbGV22vf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These test sentences need to be vectorised:"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "DUSoAyOS22vg",
        "colab_type": "code",
        "outputId": "ed64225c-af56-4d16-e01d-887909971253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "cell_type": "code",
      "source": [
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "    test_samples_X.append(s_int)\n",
        " \n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "print(test_samples_X)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1947 1538  307 3351 7766 3060 6831    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [6467 2016 1947 1313 4424 7766 6040 2358 6831    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "Y6rLuQb322vl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And the predictions are ..."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "r5Qgjrq622vn",
        "colab_type": "code",
        "outputId": "0381640b-32d1-492a-ec60-e4f990315b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_samples_X)\n",
        "print(predictions, predictions.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[3.76531594e-02 1.47297652e-02 2.50692926e-02 ... 1.71638988e-02\n",
            "   3.13185081e-02 2.01013293e-02]\n",
            "  [3.76450792e-02 1.02725299e-02 2.28186492e-02 ... 1.47819789e-02\n",
            "   3.41623910e-02 2.07608156e-02]\n",
            "  [3.43763158e-02 5.01843169e-03 2.02158727e-02 ... 1.14209661e-02\n",
            "   4.11723331e-02 2.14138813e-02]\n",
            "  ...\n",
            "  [9.99945760e-01 2.47379238e-13 1.26558264e-10 ... 1.80510606e-11\n",
            "   9.91440288e-07 5.38341108e-11]\n",
            "  [9.99945760e-01 2.47376880e-13 1.26557778e-10 ... 1.80509583e-11\n",
            "   9.91438355e-07 5.38339026e-11]\n",
            "  [9.99945879e-01 2.47375037e-13 1.26557057e-10 ... 1.80507883e-11\n",
            "   9.91434717e-07 5.38336008e-11]]\n",
            "\n",
            " [[2.73831058e-02 1.55562880e-02 2.54227985e-02 ... 1.75938290e-02\n",
            "   2.91947685e-02 2.06104014e-02]\n",
            "  [3.70595083e-02 1.03689255e-02 2.79295594e-02 ... 1.45388581e-02\n",
            "   3.85050736e-02 1.98225062e-02]\n",
            "  [6.10151142e-02 4.14086645e-03 2.80714016e-02 ... 1.02367019e-02\n",
            "   5.41988276e-02 1.89223159e-02]\n",
            "  ...\n",
            "  [9.99945879e-01 2.47275019e-13 1.26526664e-10 ... 1.80434886e-11\n",
            "   9.91287266e-07 5.38207673e-11]\n",
            "  [9.99945879e-01 2.47274071e-13 1.26526428e-10 ... 1.80434209e-11\n",
            "   9.91286356e-07 5.38206667e-11]\n",
            "  [9.99945879e-01 2.47272201e-13 1.26525943e-10 ... 1.80433186e-11\n",
            "   9.91285447e-07 5.38204620e-11]]] (2, 271, 47)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "hYyJkHb_22vt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These predictions show, for each word, the probabilities assigned to each possible label. Let's choose the label with the highest probability (using numpy's `argmax`) and then convert from label index to label:"
      ]
    },
    {
      "metadata": {
        "id": "s1EEpd2K22vv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "P-vgNHpt22vz",
        "colab_type": "code",
        "outputId": "b0692b26-2085-4c1b-dc8c-4ff32593957a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['-PAD-', 'NNP', 'NNP', 'NN', 'IN', 'NNS', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['DT', 'DT', '-PAD-', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "xUAVFac_22v6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As expected, the padding symbol is over-used. To solve this problem we can define a custom accuracy metric that ignores the paddings. Remember that the index for the padding symbol is 0:"
      ]
    },
    {
      "metadata": {
        "id": "htLSFx1722v8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        " \n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "owboYAh022v_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now retrain using this new accuracy metric:"
      ]
    },
    {
      "metadata": {
        "id": "Adx9EbTB22wC",
        "colab_type": "code",
        "outputId": "d547c2d4-bf91-4c91-c219-5b229f31b067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 271, 128)          1111424   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 271, 256)          394240    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 271, 47)           12079     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 1,517,743\n",
            "Trainable params: 1,517,743\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "vglktjZL22wI",
        "colab_type": "code",
        "outputId": "da3f6795-596d-4522-b33f-924fb2a72728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3635
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=100, validation_split=0.2) #,\n",
        "#                    callbacks=[EarlyStopping(monitor='val_ignore_accuracy')])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1878 samples, validate on 470 samples\n",
            "Epoch 1/100\n",
            "1878/1878 [==============================] - 10s 5ms/step - loss: 1.8117 - acc: 0.8438 - ignore_accuracy: 0.0062 - val_loss: 0.6134 - val_acc: 0.9036 - val_ignore_accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.5924 - acc: 0.8922 - ignore_accuracy: 0.0467 - val_loss: 0.4488 - val_acc: 0.9039 - val_ignore_accuracy: 0.1309\n",
            "Epoch 3/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.4093 - acc: 0.9061 - ignore_accuracy: 0.1774 - val_loss: 0.4021 - val_acc: 0.9041 - val_ignore_accuracy: 0.1892\n",
            "Epoch 4/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3896 - acc: 0.9061 - ignore_accuracy: 0.2106 - val_loss: 0.3980 - val_acc: 0.9039 - val_ignore_accuracy: 0.2012\n",
            "Epoch 5/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3749 - acc: 0.9059 - ignore_accuracy: 0.1553 - val_loss: 0.3839 - val_acc: 0.9039 - val_ignore_accuracy: 0.1400\n",
            "Epoch 6/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3637 - acc: 0.9060 - ignore_accuracy: 0.1498 - val_loss: 0.3794 - val_acc: 0.9039 - val_ignore_accuracy: 0.0923\n",
            "Epoch 7/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3555 - acc: 0.9060 - ignore_accuracy: 0.1210 - val_loss: 0.3752 - val_acc: 0.9039 - val_ignore_accuracy: 0.0732\n",
            "Epoch 8/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3466 - acc: 0.9061 - ignore_accuracy: 0.1141 - val_loss: 0.3799 - val_acc: 0.9040 - val_ignore_accuracy: 0.0744\n",
            "Epoch 9/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3384 - acc: 0.9063 - ignore_accuracy: 0.1120 - val_loss: 0.3682 - val_acc: 0.9041 - val_ignore_accuracy: 0.0718\n",
            "Epoch 10/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3313 - acc: 0.9069 - ignore_accuracy: 0.1147 - val_loss: 0.3688 - val_acc: 0.9053 - val_ignore_accuracy: 0.1151\n",
            "Epoch 11/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3258 - acc: 0.9091 - ignore_accuracy: 0.1200 - val_loss: 0.3652 - val_acc: 0.9083 - val_ignore_accuracy: 0.1279\n",
            "Epoch 12/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3238 - acc: 0.9115 - ignore_accuracy: 0.1246 - val_loss: 0.3485 - val_acc: 0.9111 - val_ignore_accuracy: 0.1292\n",
            "Epoch 13/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3238 - acc: 0.9132 - ignore_accuracy: 0.1270 - val_loss: 0.3644 - val_acc: 0.9103 - val_ignore_accuracy: 0.1270\n",
            "Epoch 14/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3155 - acc: 0.9135 - ignore_accuracy: 0.1262 - val_loss: 0.3506 - val_acc: 0.9121 - val_ignore_accuracy: 0.1316\n",
            "Epoch 15/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3164 - acc: 0.9145 - ignore_accuracy: 0.1296 - val_loss: 0.3692 - val_acc: 0.9109 - val_ignore_accuracy: 0.1289\n",
            "Epoch 16/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3093 - acc: 0.9147 - ignore_accuracy: 0.1290 - val_loss: 0.3444 - val_acc: 0.9132 - val_ignore_accuracy: 0.1345\n",
            "Epoch 17/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3053 - acc: 0.9157 - ignore_accuracy: 0.1318 - val_loss: 0.3532 - val_acc: 0.9135 - val_ignore_accuracy: 0.1349\n",
            "Epoch 18/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3013 - acc: 0.9162 - ignore_accuracy: 0.1328 - val_loss: 0.3613 - val_acc: 0.9140 - val_ignore_accuracy: 0.1392\n",
            "Epoch 19/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2976 - acc: 0.9170 - ignore_accuracy: 0.1378 - val_loss: 0.3683 - val_acc: 0.9146 - val_ignore_accuracy: 0.1436\n",
            "Epoch 20/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2944 - acc: 0.9181 - ignore_accuracy: 0.1472 - val_loss: 0.3776 - val_acc: 0.9150 - val_ignore_accuracy: 0.1481\n",
            "Epoch 21/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2917 - acc: 0.9194 - ignore_accuracy: 0.1584 - val_loss: 0.3835 - val_acc: 0.9154 - val_ignore_accuracy: 0.1525\n",
            "Epoch 22/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2899 - acc: 0.9200 - ignore_accuracy: 0.1644 - val_loss: 0.3197 - val_acc: 0.9141 - val_ignore_accuracy: 0.1457\n",
            "Epoch 23/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2953 - acc: 0.9197 - ignore_accuracy: 0.1653 - val_loss: 0.3090 - val_acc: 0.9194 - val_ignore_accuracy: 0.1759\n",
            "Epoch 24/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2864 - acc: 0.9207 - ignore_accuracy: 0.1711 - val_loss: 0.3081 - val_acc: 0.9202 - val_ignore_accuracy: 0.1857\n",
            "Epoch 25/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2826 - acc: 0.9224 - ignore_accuracy: 0.1876 - val_loss: 0.3077 - val_acc: 0.9212 - val_ignore_accuracy: 0.1960\n",
            "Epoch 26/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2794 - acc: 0.9233 - ignore_accuracy: 0.1956 - val_loss: 0.3163 - val_acc: 0.9211 - val_ignore_accuracy: 0.1984\n",
            "Epoch 27/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2764 - acc: 0.9234 - ignore_accuracy: 0.1962 - val_loss: 0.3137 - val_acc: 0.9212 - val_ignore_accuracy: 0.1985\n",
            "Epoch 28/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2734 - acc: 0.9236 - ignore_accuracy: 0.1970 - val_loss: 0.3248 - val_acc: 0.9211 - val_ignore_accuracy: 0.2025\n",
            "Epoch 29/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2703 - acc: 0.9245 - ignore_accuracy: 0.2067 - val_loss: 0.3140 - val_acc: 0.9220 - val_ignore_accuracy: 0.2087\n",
            "Epoch 30/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2667 - acc: 0.9254 - ignore_accuracy: 0.2152 - val_loss: 0.3111 - val_acc: 0.9230 - val_ignore_accuracy: 0.2199\n",
            "Epoch 31/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2627 - acc: 0.9271 - ignore_accuracy: 0.2332 - val_loss: 0.3102 - val_acc: 0.9244 - val_ignore_accuracy: 0.2374\n",
            "Epoch 32/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2578 - acc: 0.9287 - ignore_accuracy: 0.2504 - val_loss: 0.2986 - val_acc: 0.9271 - val_ignore_accuracy: 0.2647\n",
            "Epoch 33/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2521 - acc: 0.9317 - ignore_accuracy: 0.2826 - val_loss: 0.2955 - val_acc: 0.9270 - val_ignore_accuracy: 0.2656\n",
            "Epoch 34/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2452 - acc: 0.9335 - ignore_accuracy: 0.3014 - val_loss: 0.2799 - val_acc: 0.9303 - val_ignore_accuracy: 0.2976\n",
            "Epoch 35/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2371 - acc: 0.9356 - ignore_accuracy: 0.3246 - val_loss: 0.2693 - val_acc: 0.9321 - val_ignore_accuracy: 0.3163\n",
            "Epoch 36/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2281 - acc: 0.9378 - ignore_accuracy: 0.3482 - val_loss: 0.2640 - val_acc: 0.9339 - val_ignore_accuracy: 0.3396\n",
            "Epoch 37/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2180 - acc: 0.9410 - ignore_accuracy: 0.3826 - val_loss: 0.2445 - val_acc: 0.9380 - val_ignore_accuracy: 0.3797\n",
            "Epoch 38/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2070 - acc: 0.9449 - ignore_accuracy: 0.4251 - val_loss: 0.2245 - val_acc: 0.9440 - val_ignore_accuracy: 0.4365\n",
            "Epoch 39/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1958 - acc: 0.9492 - ignore_accuracy: 0.4708 - val_loss: 0.2158 - val_acc: 0.9463 - val_ignore_accuracy: 0.4652\n",
            "Epoch 40/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1841 - acc: 0.9533 - ignore_accuracy: 0.5144 - val_loss: 0.2048 - val_acc: 0.9491 - val_ignore_accuracy: 0.4949\n",
            "Epoch 41/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.5448 - acc: 0.8959 - ignore_accuracy: 0.5117 - val_loss: 0.2227 - val_acc: 0.9511 - val_ignore_accuracy: 0.5035\n",
            "Epoch 42/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1996 - acc: 0.9502 - ignore_accuracy: 0.4968 - val_loss: 0.2030 - val_acc: 0.9510 - val_ignore_accuracy: 0.5072\n",
            "Epoch 43/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1835 - acc: 0.9537 - ignore_accuracy: 0.5357 - val_loss: 0.1876 - val_acc: 0.9521 - val_ignore_accuracy: 0.5380\n",
            "Epoch 44/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1706 - acc: 0.9581 - ignore_accuracy: 0.5796 - val_loss: 0.1774 - val_acc: 0.9572 - val_ignore_accuracy: 0.5750\n",
            "Epoch 45/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1617 - acc: 0.9611 - ignore_accuracy: 0.6066 - val_loss: 0.1701 - val_acc: 0.9597 - val_ignore_accuracy: 0.5996\n",
            "Epoch 46/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1543 - acc: 0.9632 - ignore_accuracy: 0.6270 - val_loss: 0.1638 - val_acc: 0.9605 - val_ignore_accuracy: 0.6074\n",
            "Epoch 47/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1477 - acc: 0.9647 - ignore_accuracy: 0.6411 - val_loss: 0.1582 - val_acc: 0.9618 - val_ignore_accuracy: 0.6199\n",
            "Epoch 48/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1416 - acc: 0.9661 - ignore_accuracy: 0.6550 - val_loss: 0.1528 - val_acc: 0.9629 - val_ignore_accuracy: 0.6302\n",
            "Epoch 49/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1358 - acc: 0.9675 - ignore_accuracy: 0.6690 - val_loss: 0.1477 - val_acc: 0.9641 - val_ignore_accuracy: 0.6421\n",
            "Epoch 50/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1302 - acc: 0.9691 - ignore_accuracy: 0.6847 - val_loss: 0.1429 - val_acc: 0.9650 - val_ignore_accuracy: 0.6510\n",
            "Epoch 51/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1250 - acc: 0.9704 - ignore_accuracy: 0.6978 - val_loss: 0.1383 - val_acc: 0.9661 - val_ignore_accuracy: 0.6600\n",
            "Epoch 52/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1199 - acc: 0.9719 - ignore_accuracy: 0.7121 - val_loss: 0.1338 - val_acc: 0.9677 - val_ignore_accuracy: 0.6757\n",
            "Epoch 53/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1150 - acc: 0.9732 - ignore_accuracy: 0.7257 - val_loss: 0.1296 - val_acc: 0.9689 - val_ignore_accuracy: 0.6872\n",
            "Epoch 54/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1103 - acc: 0.9747 - ignore_accuracy: 0.7407 - val_loss: 0.1256 - val_acc: 0.9703 - val_ignore_accuracy: 0.7009\n",
            "Epoch 55/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1058 - acc: 0.9759 - ignore_accuracy: 0.7535 - val_loss: 0.1216 - val_acc: 0.9715 - val_ignore_accuracy: 0.7126\n",
            "Epoch 56/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1013 - acc: 0.9775 - ignore_accuracy: 0.7694 - val_loss: 0.1178 - val_acc: 0.9728 - val_ignore_accuracy: 0.7249\n",
            "Epoch 57/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0970 - acc: 0.9788 - ignore_accuracy: 0.7821 - val_loss: 0.1142 - val_acc: 0.9734 - val_ignore_accuracy: 0.7314\n",
            "Epoch 58/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0929 - acc: 0.9799 - ignore_accuracy: 0.7928 - val_loss: 0.1106 - val_acc: 0.9746 - val_ignore_accuracy: 0.7422\n",
            "Epoch 59/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0888 - acc: 0.9809 - ignore_accuracy: 0.8033 - val_loss: 0.1072 - val_acc: 0.9754 - val_ignore_accuracy: 0.7502\n",
            "Epoch 60/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0849 - acc: 0.9819 - ignore_accuracy: 0.8131 - val_loss: 0.1038 - val_acc: 0.9763 - val_ignore_accuracy: 0.7598\n",
            "Epoch 61/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0810 - acc: 0.9829 - ignore_accuracy: 0.8233 - val_loss: 0.1006 - val_acc: 0.9770 - val_ignore_accuracy: 0.7669\n",
            "Epoch 62/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0774 - acc: 0.9837 - ignore_accuracy: 0.8318 - val_loss: 0.0975 - val_acc: 0.9777 - val_ignore_accuracy: 0.7728\n",
            "Epoch 63/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0738 - acc: 0.9846 - ignore_accuracy: 0.8404 - val_loss: 0.0945 - val_acc: 0.9783 - val_ignore_accuracy: 0.7800\n",
            "Epoch 64/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0704 - acc: 0.9854 - ignore_accuracy: 0.8487 - val_loss: 0.0917 - val_acc: 0.9791 - val_ignore_accuracy: 0.7874\n",
            "Epoch 65/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0671 - acc: 0.9863 - ignore_accuracy: 0.8583 - val_loss: 0.0891 - val_acc: 0.9799 - val_ignore_accuracy: 0.7949\n",
            "Epoch 66/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0640 - acc: 0.9871 - ignore_accuracy: 0.8671 - val_loss: 0.0866 - val_acc: 0.9804 - val_ignore_accuracy: 0.8004\n",
            "Epoch 67/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0610 - acc: 0.9879 - ignore_accuracy: 0.8752 - val_loss: 0.0840 - val_acc: 0.9810 - val_ignore_accuracy: 0.8058\n",
            "Epoch 68/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0581 - acc: 0.9886 - ignore_accuracy: 0.8817 - val_loss: 0.0817 - val_acc: 0.9815 - val_ignore_accuracy: 0.8106\n",
            "Epoch 69/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0553 - acc: 0.9892 - ignore_accuracy: 0.8885 - val_loss: 0.0795 - val_acc: 0.9822 - val_ignore_accuracy: 0.8180\n",
            "Epoch 70/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0527 - acc: 0.9899 - ignore_accuracy: 0.8955 - val_loss: 0.0776 - val_acc: 0.9827 - val_ignore_accuracy: 0.8230\n",
            "Epoch 71/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0502 - acc: 0.9905 - ignore_accuracy: 0.9016 - val_loss: 0.0755 - val_acc: 0.9832 - val_ignore_accuracy: 0.8282\n",
            "Epoch 72/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0478 - acc: 0.9909 - ignore_accuracy: 0.9057 - val_loss: 0.0737 - val_acc: 0.9835 - val_ignore_accuracy: 0.8312\n",
            "Epoch 73/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0456 - acc: 0.9913 - ignore_accuracy: 0.9101 - val_loss: 0.0721 - val_acc: 0.9840 - val_ignore_accuracy: 0.8358\n",
            "Epoch 74/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0435 - acc: 0.9919 - ignore_accuracy: 0.9157 - val_loss: 0.0707 - val_acc: 0.9844 - val_ignore_accuracy: 0.8401\n",
            "Epoch 75/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0415 - acc: 0.9922 - ignore_accuracy: 0.9194 - val_loss: 0.0690 - val_acc: 0.9846 - val_ignore_accuracy: 0.8423\n",
            "Epoch 76/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0397 - acc: 0.9926 - ignore_accuracy: 0.9228 - val_loss: 0.0676 - val_acc: 0.9848 - val_ignore_accuracy: 0.8450\n",
            "Epoch 77/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0379 - acc: 0.9930 - ignore_accuracy: 0.9267 - val_loss: 0.0663 - val_acc: 0.9851 - val_ignore_accuracy: 0.8475\n",
            "Epoch 78/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0362 - acc: 0.9933 - ignore_accuracy: 0.9298 - val_loss: 0.0650 - val_acc: 0.9852 - val_ignore_accuracy: 0.8489\n",
            "Epoch 79/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0347 - acc: 0.9936 - ignore_accuracy: 0.9328 - val_loss: 0.0640 - val_acc: 0.9855 - val_ignore_accuracy: 0.8515\n",
            "Epoch 80/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0332 - acc: 0.9938 - ignore_accuracy: 0.9356 - val_loss: 0.0631 - val_acc: 0.9856 - val_ignore_accuracy: 0.8531\n",
            "Epoch 81/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0318 - acc: 0.9940 - ignore_accuracy: 0.9378 - val_loss: 0.0621 - val_acc: 0.9858 - val_ignore_accuracy: 0.8545\n",
            "Epoch 82/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0305 - acc: 0.9944 - ignore_accuracy: 0.9408 - val_loss: 0.0611 - val_acc: 0.9859 - val_ignore_accuracy: 0.8557\n",
            "Epoch 83/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0294 - acc: 0.9945 - ignore_accuracy: 0.9426 - val_loss: 0.0602 - val_acc: 0.9861 - val_ignore_accuracy: 0.8577\n",
            "Epoch 84/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0282 - acc: 0.9947 - ignore_accuracy: 0.9444 - val_loss: 0.0594 - val_acc: 0.9863 - val_ignore_accuracy: 0.8596\n",
            "Epoch 85/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0271 - acc: 0.9949 - ignore_accuracy: 0.9467 - val_loss: 0.0588 - val_acc: 0.9864 - val_ignore_accuracy: 0.8612\n",
            "Epoch 86/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0261 - acc: 0.9951 - ignore_accuracy: 0.9485 - val_loss: 0.0583 - val_acc: 0.9865 - val_ignore_accuracy: 0.8613\n",
            "Epoch 87/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0251 - acc: 0.9953 - ignore_accuracy: 0.9505 - val_loss: 0.0575 - val_acc: 0.9867 - val_ignore_accuracy: 0.8633\n",
            "Epoch 88/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0242 - acc: 0.9954 - ignore_accuracy: 0.9520 - val_loss: 0.0572 - val_acc: 0.9869 - val_ignore_accuracy: 0.8660\n",
            "Epoch 89/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0234 - acc: 0.9957 - ignore_accuracy: 0.9548 - val_loss: 0.0565 - val_acc: 0.9870 - val_ignore_accuracy: 0.8665\n",
            "Epoch 90/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0226 - acc: 0.9959 - ignore_accuracy: 0.9569 - val_loss: 0.0561 - val_acc: 0.9871 - val_ignore_accuracy: 0.8669\n",
            "Epoch 91/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0218 - acc: 0.9960 - ignore_accuracy: 0.9577 - val_loss: 0.0556 - val_acc: 0.9872 - val_ignore_accuracy: 0.8682\n",
            "Epoch 92/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0211 - acc: 0.9961 - ignore_accuracy: 0.9591 - val_loss: 0.0551 - val_acc: 0.9872 - val_ignore_accuracy: 0.8689\n",
            "Epoch 93/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0204 - acc: 0.9962 - ignore_accuracy: 0.9603 - val_loss: 0.0546 - val_acc: 0.9872 - val_ignore_accuracy: 0.8687\n",
            "Epoch 94/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0198 - acc: 0.9963 - ignore_accuracy: 0.9615 - val_loss: 0.0544 - val_acc: 0.9874 - val_ignore_accuracy: 0.8704\n",
            "Epoch 95/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0192 - acc: 0.9964 - ignore_accuracy: 0.9623 - val_loss: 0.0542 - val_acc: 0.9873 - val_ignore_accuracy: 0.8696\n",
            "Epoch 96/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0186 - acc: 0.9965 - ignore_accuracy: 0.9632 - val_loss: 0.0539 - val_acc: 0.9874 - val_ignore_accuracy: 0.8709\n",
            "Epoch 97/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0180 - acc: 0.9966 - ignore_accuracy: 0.9641 - val_loss: 0.0535 - val_acc: 0.9875 - val_ignore_accuracy: 0.8709\n",
            "Epoch 98/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0175 - acc: 0.9967 - ignore_accuracy: 0.9649 - val_loss: 0.0534 - val_acc: 0.9875 - val_ignore_accuracy: 0.8709\n",
            "Epoch 99/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0170 - acc: 0.9968 - ignore_accuracy: 0.9658 - val_loss: 0.0529 - val_acc: 0.9875 - val_ignore_accuracy: 0.8710\n",
            "Epoch 100/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0166 - acc: 0.9969 - ignore_accuracy: 0.9667 - val_loss: 0.0534 - val_acc: 0.9876 - val_ignore_accuracy: 0.8722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "fr0DO2e522wU",
        "colab_type": "code",
        "outputId": "ad5adc48-7c6d-4222-ccec-a38dccaf7231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_samples_X)\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['MD', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "dWsUFxFb22wZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see the great difference in accuracy when we ignore the padding symbol. Also, note that the results reported by the original post https://nlpforhackers.io/lstm-pos-tagger-keras/ are better, probably because they used two LSTM chains: one going forwards, and another going backwards. This is what is called a **bidirectional recurrent network**, and it usually reports better results because each prediction is based on the context on the left and the right of the token."
      ]
    },
    {
      "metadata": {
        "id": "QFJouksxEsDM",
        "colab_type": "code",
        "outputId": "942bacca-6055-4030-8d8e-bca6f3882c2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'val_ignore_accuracy', 'loss', 'acc', 'ignore_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "L_ieDH5d22wb",
        "colab_type": "code",
        "outputId": "40ec346f-d89d-4177-f90b-98a5756c02a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [20, 5]\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "ignore_acc = history.history['ignore_accuracy']\n",
        "val_ignore_acc = history.history['val_ignore_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.plot(epochs, ignore_acc, 'bo', label='Training ignore acc')\n",
        "plt.plot(epochs, val_ignore_acc, 'b', label='Validation ignore acc')\n",
        "plt.title('Training and validation ignore accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIUAAAE+CAYAAAATeaC8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9P8BdK4uMmmg9B8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x360 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BPsYClKQFTtc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}