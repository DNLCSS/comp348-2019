{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a Part of Speech Tagger with Keras\n",
    "\n",
    "This notebook is based on nlpforhackers' post: https://nlpforhackers.io/lstm-pos-tagger-keras/. We will look at how to build a part of speech tagger using a LSTM layer.\n",
    "\n",
    "We will use NLTK's treebank corpus. This corpus has, among other kinds of information, annotations about the parts of speech of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "Tagged sentences:  3914\n",
      "Tagged words: 100676\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    " \n",
    "print(tagged_sentences[0])\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As always, before training a model, we need to split the data in training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234)\n",
    "tagged_sentences = list(tagged_sentences) # we convert the data to a list\n",
    "random.shuffle(tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(len(tagged_sentences)*.6)\n",
    "train = tagged_sentences[:threshold]\n",
    "test = tagged_sentences[threshold:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s restructure the data a bit. Let’s separate the words from the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "train_sentences, train_tags =[], [] \n",
    "for tagged_sentence in train:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    train_sentences.append(np.array(sentence))\n",
    "    train_tags.append(np.array(tags))\n",
    "    \n",
    "test_sentences, test_tags =[], [] \n",
    "for tagged_sentence in test:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    test_sentences.append(np.array(sentence))\n",
    "    test_tags.append(np.array(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They' 'are' 'keeping' 'a' 'close' 'watch' 'on' 'the' 'yield' 'on' 'the'\n",
      " 'S&P' '500' '.']\n",
      "['PRP' 'VBP' 'VBG' 'DT' 'JJ' 'NN' 'IN' 'DT' 'NN' 'IN' 'DT' 'NNP' 'CD' '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[5])\n",
    "print(train_tags[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now convert the words and tags to indices. We will reserve index 0 for padding, and index 1 for words that are out of the vocabulary (**OOV - Out of Vocabulary**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    " \n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    " \n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding\n",
    "tag2index['-OOV-'] = 1  # There may also be unknown tags in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[754, 6164, 7542, 7659, 7464, 102, 8463, 8244]\n",
      "[4138, 8525, 3112, 4960, 4849, 1, 1147, 439, 7282, 1408, 5386, 439, 1, 1592, 7789, 7464, 6913, 8095, 439, 8621, 5896, 1, 8244]\n",
      "[11, 15, 9, 29, 7, 43, 43, 45]\n",
      "[41, 29, 4, 20, 29, 4, 31, 38, 35, 41, 4, 38, 4, 31, 4, 7, 10, 4, 38, 31, 42, 4, 45]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    " \n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    " \n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    " \n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    " \n",
    "for s in test_tags:\n",
    "    s_int = []\n",
    "    for t in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[t])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    test_tags_y.append(s_int)\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since Keras can only deal with fixed size sequences, we will pad all sequences to fit the longest sentence in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 754 6164 7542 7659 7464  102 8463 8244    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[4138 8525 3112 4960 4849    1 1147  439 7282 1408 5386  439    1 1592\n",
      " 7789 7464 6913 8095  439 8621 5896    1 8244    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[11 15  9 29  7 43 43 45  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n",
      "[41 29  4 20 29  4 31 38 35 41  4 38  4 31  4  7 10  4 38 31 42  4 45  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now define the model. \n",
    "A simple approach to build a part-of-speech tagger in Keras is to stack a dense layer at the output of each RNN cell. This is achieved with `TimeDistributed`. Note  how the output of the model is now a list of sequences of vectors. Each vector will contain the probability of each tag. In particular:\n",
    "\n",
    "* We will introduce an embedding layer.\n",
    "* THe LSTM layer will have the parameter `return_sequences=True` so that we have access to the entire sequence.\n",
    "* The output of each LSTM cell will have a `Dense` layer. We can do this using the `TimeDistributed` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 271, 128)          1111424   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 271, 256)          394240    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 271, 47)           12079     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 271, 47)           0         \n",
      "=================================================================\n",
      "Total params: 1,517,743\n",
      "Trainable params: 1,517,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also need to convert the labels into one-hot encoding. Since the input are sequences of labels we cannot use Keras' `to_categorical` so we define our own function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print(cat_train_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can finally train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1878 samples, validate on 470 samples\n",
      "Epoch 1/40\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 1.7629 - acc: 0.8446 - val_loss: 0.6247 - val_acc: 0.9036\n",
      "Epoch 2/40\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.6393 - acc: 0.8875 - val_loss: 0.4856 - val_acc: 0.9038\n",
      "Epoch 3/40\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.4455 - acc: 0.9062 - val_loss: 0.4177 - val_acc: 0.9057\n",
      "Epoch 4/40\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.3990 - acc: 0.9086 - val_loss: 0.3997 - val_acc: 0.9047\n",
      "Epoch 5/40\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.3768 - acc: 0.9077 - val_loss: 0.3772 - val_acc: 0.9060\n",
      "Epoch 6/40\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.3619 - acc: 0.9078 - val_loss: 0.3599 - val_acc: 0.9058\n",
      "Epoch 7/40\n",
      "1878/1878 [==============================] - 23s 12ms/step - loss: 0.3536 - acc: 0.9079 - val_loss: 0.3536 - val_acc: 0.9066\n",
      "Epoch 8/40\n",
      "1878/1878 [==============================] - 22s 12ms/step - loss: 0.3421 - acc: 0.9099 - val_loss: 0.3416 - val_acc: 0.9099\n",
      "Epoch 9/40\n",
      "1878/1878 [==============================] - 23s 12ms/step - loss: 0.3318 - acc: 0.9122 - val_loss: 0.3436 - val_acc: 0.9129\n",
      "Epoch 10/40\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.3266 - acc: 0.9126 - val_loss: 0.3270 - val_acc: 0.9121\n",
      "Epoch 11/40\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.3166 - acc: 0.9136 - val_loss: 0.3201 - val_acc: 0.9130\n",
      "Epoch 12/40\n",
      "1878/1878 [==============================] - 20s 10ms/step - loss: 0.3093 - acc: 0.9148 - val_loss: 0.3148 - val_acc: 0.9141\n",
      "Epoch 13/40\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.3031 - acc: 0.9161 - val_loss: 0.3109 - val_acc: 0.9167\n",
      "Epoch 14/40\n",
      "1878/1878 [==============================] - 25s 13ms/step - loss: 0.2994 - acc: 0.9185 - val_loss: 0.3155 - val_acc: 0.9181\n",
      "Epoch 15/40\n",
      "1878/1878 [==============================] - 22s 12ms/step - loss: 0.7726 - acc: 0.8434 - val_loss: 0.3803 - val_acc: 0.9082\n",
      "Epoch 16/40\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.3489 - acc: 0.9149 - val_loss: 0.3520 - val_acc: 0.9161\n",
      "Epoch 17/40\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.3418 - acc: 0.9188 - val_loss: 0.3458 - val_acc: 0.9172\n",
      "Epoch 18/40\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.3341 - acc: 0.9193 - val_loss: 0.3371 - val_acc: 0.9175\n",
      "Epoch 19/40\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.3239 - acc: 0.9193 - val_loss: 0.3249 - val_acc: 0.9179\n",
      "Epoch 20/40\n",
      "1878/1878 [==============================] - 22s 11ms/step - loss: 0.3093 - acc: 0.9189 - val_loss: 0.3082 - val_acc: 0.9180\n",
      "Epoch 21/40\n",
      "1878/1878 [==============================] - 24s 13ms/step - loss: 0.2952 - acc: 0.9206 - val_loss: 0.2988 - val_acc: 0.9209\n",
      "Epoch 22/40\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.2892 - acc: 0.9210 - val_loss: 0.2951 - val_acc: 0.9198\n",
      "Epoch 23/40\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.2859 - acc: 0.9210 - val_loss: 0.2923 - val_acc: 0.9198\n",
      "Epoch 24/40\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.2834 - acc: 0.9211 - val_loss: 0.2903 - val_acc: 0.9198\n",
      "Epoch 25/40\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.2814 - acc: 0.9212 - val_loss: 0.2886 - val_acc: 0.9205\n",
      "Epoch 26/40\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.2796 - acc: 0.9218 - val_loss: 0.2873 - val_acc: 0.9206\n",
      "Epoch 27/40\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.2780 - acc: 0.9221 - val_loss: 0.2862 - val_acc: 0.9209\n",
      "Epoch 28/40\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.2764 - acc: 0.9223 - val_loss: 0.2849 - val_acc: 0.9210\n",
      "Epoch 29/40\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.2748 - acc: 0.9226 - val_loss: 0.2837 - val_acc: 0.9213\n",
      "Epoch 30/40\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.2733 - acc: 0.9227 - val_loss: 0.2824 - val_acc: 0.9215\n",
      "Epoch 31/40\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.2716 - acc: 0.9227 - val_loss: 0.2809 - val_acc: 0.9218\n",
      "Epoch 32/40\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.2698 - acc: 0.9228 - val_loss: 0.2793 - val_acc: 0.9219\n",
      "Epoch 33/40\n",
      "1878/1878 [==============================] - 17s 9ms/step - loss: 0.2679 - acc: 0.9229 - val_loss: 0.2775 - val_acc: 0.9218\n",
      "Epoch 34/40\n",
      "1878/1878 [==============================] - 17s 9ms/step - loss: 0.2657 - acc: 0.9231 - val_loss: 0.2754 - val_acc: 0.9224\n",
      "Epoch 35/40\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.2632 - acc: 0.9240 - val_loss: 0.2730 - val_acc: 0.9236\n",
      "Epoch 36/40\n",
      "1878/1878 [==============================] - 18s 9ms/step - loss: 0.2604 - acc: 0.9254 - val_loss: 0.2703 - val_acc: 0.9250\n",
      "Epoch 37/40\n",
      "1878/1878 [==============================] - 17s 9ms/step - loss: 0.2571 - acc: 0.9270 - val_loss: 0.2671 - val_acc: 0.9265\n",
      "Epoch 38/40\n",
      "1878/1878 [==============================] - 17s 9ms/step - loss: 0.2533 - acc: 0.9286 - val_loss: 0.2633 - val_acc: 0.9275\n",
      "Epoch 39/40\n",
      "1878/1878 [==============================] - 17s 9ms/step - loss: 0.2488 - acc: 0.9304 - val_loss: 0.2585 - val_acc: 0.9290\n",
      "Epoch 40/40\n",
      "1878/1878 [==============================] - 17s 9ms/step - loss: 0.2436 - acc: 0.9322 - val_loss: 0.2534 - val_acc: 0.9309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0e15dfdfd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_split=0.2)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the results look too good, it's because they are too good! We need to ignore the classification of the padded symbols, but let's leave it aside for now. Below is an evaluation using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 6s 4ms/step\n",
      "acc: 93.28064533333517\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's find the PoS of two test sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n"
     ]
    }
   ],
   "source": [
    "test_samples = [\n",
    "    \"running is very important for me .\".split(),\n",
    "    \"I was running every day for a month .\".split()\n",
    "]\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These test sentences need to be vectorised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5416 3046  879  225 7049 5468 8244    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]\n",
      " [7529 8148 5416 2708 1888 7049 4270 4411 8244    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "test_samples_X = []\n",
    "for s in test_samples:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    test_samples_X.append(s_int)\n",
    " \n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "print(test_samples_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And the predictions are ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4.4955608e-02 1.5484265e-02 2.2056568e-02 ... 1.8686477e-02\n",
      "   4.4350747e-02 1.8839989e-02]\n",
      "  [6.1708014e-02 1.3672160e-02 2.2489037e-02 ... 1.5864795e-02\n",
      "   4.0098306e-02 1.4878591e-02]\n",
      "  [9.4220236e-02 1.0509712e-02 2.2016199e-02 ... 1.2349819e-02\n",
      "   4.6605412e-02 1.0682364e-02]\n",
      "  ...\n",
      "  [9.9996805e-01 1.2200499e-10 7.6720987e-08 ... 7.1617540e-10\n",
      "   3.5734286e-06 3.6779796e-10]\n",
      "  [9.9996805e-01 1.2200499e-10 7.6720987e-08 ... 7.1617540e-10\n",
      "   3.5734286e-06 3.6779796e-10]\n",
      "  [9.9996805e-01 1.2200523e-10 7.6721136e-08 ... 7.1617401e-10\n",
      "   3.5734354e-06 3.6780007e-10]]\n",
      "\n",
      " [[1.9523820e-02 2.0028079e-02 2.1672221e-02 ... 1.8764738e-02\n",
      "   1.5450116e-02 1.7119510e-02]\n",
      "  [2.2560960e-02 1.6599175e-02 2.3360679e-02 ... 1.4789831e-02\n",
      "   1.2849574e-02 1.0460691e-02]\n",
      "  [4.8549145e-02 1.2866377e-02 2.6165031e-02 ... 1.4345896e-02\n",
      "   3.7202392e-02 1.0042745e-02]\n",
      "  ...\n",
      "  [9.9996805e-01 1.2200499e-10 7.6720987e-08 ... 7.1617540e-10\n",
      "   3.5734286e-06 3.6779796e-10]\n",
      "  [9.9996805e-01 1.2200499e-10 7.6720987e-08 ... 7.1617540e-10\n",
      "   3.5734286e-06 3.6779796e-10]\n",
      "  [9.9996805e-01 1.2200523e-10 7.6721136e-08 ... 7.1617401e-10\n",
      "   3.5734354e-06 3.6780007e-10]]] (2, 271, 47)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_samples_X)\n",
    "print(predictions, predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These predictions show, for each word, the probabilities assigned to each possible label. Let's choose the label with the highest probability (using numpy's `argmax`) and then convert from label index to label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-PAD-', '-PAD-', '-PAD-', '-PAD-', 'IN', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['DT', 'DT', '-PAD-', 'DT', 'NN', 'NN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As expected, the padding symbol is over-used. To solve this problem we can define a custom accuracy metric that ignores the paddings. Remember that the index for the padding symbol is 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    " \n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now retrain using this new accuracy metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 271, 128)          1111424   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 271, 256)          394240    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 271, 47)           12079     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 271, 47)           0         \n",
      "=================================================================\n",
      "Total params: 1,517,743\n",
      "Trainable params: 1,517,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1878 samples, validate on 470 samples\n",
      "Epoch 1/100\n",
      "1878/1878 [==============================] - 20s 10ms/step - loss: 1.7266 - acc: 0.8440 - ignore_accuracy: 0.0018 - val_loss: 0.6099 - val_acc: 0.9036 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.6572 - acc: 0.8780 - ignore_accuracy: 0.0543 - val_loss: 0.4371 - val_acc: 0.9037 - val_ignore_accuracy: 0.1100\n",
      "Epoch 3/100\n",
      "1878/1878 [==============================] - 24s 13ms/step - loss: 0.3991 - acc: 0.9061 - ignore_accuracy: 0.1153 - val_loss: 0.3948 - val_acc: 0.9042 - val_ignore_accuracy: 0.1263\n",
      "Epoch 4/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.3802 - acc: 0.9062 - ignore_accuracy: 0.1187 - val_loss: 0.3920 - val_acc: 0.9039 - val_ignore_accuracy: 0.1097\n",
      "Epoch 5/100\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.3695 - acc: 0.9062 - ignore_accuracy: 0.1177 - val_loss: 0.3739 - val_acc: 0.9040 - val_ignore_accuracy: 0.1043\n",
      "Epoch 6/100\n",
      "1878/1878 [==============================] - 28s 15ms/step - loss: 0.3581 - acc: 0.9062 - ignore_accuracy: 0.1085 - val_loss: 0.3662 - val_acc: 0.9040 - val_ignore_accuracy: 0.0913\n",
      "Epoch 7/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.3486 - acc: 0.9062 - ignore_accuracy: 0.1048 - val_loss: 0.3586 - val_acc: 0.9041 - val_ignore_accuracy: 0.0763\n",
      "Epoch 8/100\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.3381 - acc: 0.9067 - ignore_accuracy: 0.1038 - val_loss: 0.3500 - val_acc: 0.9061 - val_ignore_accuracy: 0.1229\n",
      "Epoch 9/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.3295 - acc: 0.9089 - ignore_accuracy: 0.1195 - val_loss: 0.3420 - val_acc: 0.9090 - val_ignore_accuracy: 0.1308\n",
      "Epoch 10/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.3230 - acc: 0.9113 - ignore_accuracy: 0.1258 - val_loss: 0.3404 - val_acc: 0.9103 - val_ignore_accuracy: 0.1353\n",
      "Epoch 11/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.3178 - acc: 0.9129 - ignore_accuracy: 0.1302 - val_loss: 0.3342 - val_acc: 0.9118 - val_ignore_accuracy: 0.1365\n",
      "Epoch 12/100\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.3137 - acc: 0.9141 - ignore_accuracy: 0.1334 - val_loss: 0.3272 - val_acc: 0.9136 - val_ignore_accuracy: 0.1378\n",
      "Epoch 13/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.3276 - acc: 0.9150 - ignore_accuracy: 0.1346 - val_loss: 0.3249 - val_acc: 0.9134 - val_ignore_accuracy: 0.1383\n",
      "Epoch 14/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.3095 - acc: 0.9144 - ignore_accuracy: 0.1332 - val_loss: 0.3135 - val_acc: 0.9141 - val_ignore_accuracy: 0.1397\n",
      "Epoch 15/100\n",
      "1878/1878 [==============================] - 26s 14ms/step - loss: 0.3025 - acc: 0.9155 - ignore_accuracy: 0.1351 - val_loss: 0.3090 - val_acc: 0.9148 - val_ignore_accuracy: 0.1434\n",
      "Epoch 16/100\n",
      "1878/1878 [==============================] - 23s 13ms/step - loss: 0.2977 - acc: 0.9162 - ignore_accuracy: 0.1380 - val_loss: 0.3057 - val_acc: 0.9163 - val_ignore_accuracy: 0.1555\n",
      "Epoch 17/100\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.2934 - acc: 0.9178 - ignore_accuracy: 0.1525 - val_loss: 0.3022 - val_acc: 0.9182 - val_ignore_accuracy: 0.1711\n",
      "Epoch 18/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.2895 - acc: 0.9194 - ignore_accuracy: 0.1655 - val_loss: 0.2992 - val_acc: 0.9188 - val_ignore_accuracy: 0.1734\n",
      "Epoch 19/100\n",
      "1878/1878 [==============================] - 24s 13ms/step - loss: 0.2860 - acc: 0.9201 - ignore_accuracy: 0.1693 - val_loss: 0.2969 - val_acc: 0.9191 - val_ignore_accuracy: 0.1741\n",
      "Epoch 20/100\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.2831 - acc: 0.9206 - ignore_accuracy: 0.1720 - val_loss: 0.2947 - val_acc: 0.9195 - val_ignore_accuracy: 0.1761\n",
      "Epoch 21/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.2806 - acc: 0.9209 - ignore_accuracy: 0.1743 - val_loss: 0.2932 - val_acc: 0.9195 - val_ignore_accuracy: 0.1750\n",
      "Epoch 22/100\n",
      "1878/1878 [==============================] - 22s 12ms/step - loss: 0.2783 - acc: 0.9214 - ignore_accuracy: 0.1772 - val_loss: 0.2919 - val_acc: 0.9202 - val_ignore_accuracy: 0.1815\n",
      "Epoch 23/100\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.2762 - acc: 0.9220 - ignore_accuracy: 0.1828 - val_loss: 0.2904 - val_acc: 0.9204 - val_ignore_accuracy: 0.1819\n",
      "Epoch 24/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.2741 - acc: 0.9225 - ignore_accuracy: 0.1874 - val_loss: 0.2882 - val_acc: 0.9208 - val_ignore_accuracy: 0.1853\n",
      "Epoch 25/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.2720 - acc: 0.9229 - ignore_accuracy: 0.1915 - val_loss: 0.2868 - val_acc: 0.9214 - val_ignore_accuracy: 0.1913\n",
      "Epoch 26/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.2698 - acc: 0.9238 - ignore_accuracy: 0.1994 - val_loss: 0.2848 - val_acc: 0.9217 - val_ignore_accuracy: 0.1929\n",
      "Epoch 27/100\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.2671 - acc: 0.9247 - ignore_accuracy: 0.2095 - val_loss: 0.2825 - val_acc: 0.9228 - val_ignore_accuracy: 0.2046\n",
      "Epoch 28/100\n",
      "1878/1878 [==============================] - 23s 12ms/step - loss: 0.2641 - acc: 0.9260 - ignore_accuracy: 0.2226 - val_loss: 0.2790 - val_acc: 0.9261 - val_ignore_accuracy: 0.2394\n",
      "Epoch 29/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.2604 - acc: 0.9287 - ignore_accuracy: 0.2514 - val_loss: 0.2775 - val_acc: 0.9270 - val_ignore_accuracy: 0.2483\n",
      "Epoch 30/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.2557 - acc: 0.9308 - ignore_accuracy: 0.2735 - val_loss: 0.2705 - val_acc: 0.9295 - val_ignore_accuracy: 0.2747\n",
      "Epoch 31/100\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.2496 - acc: 0.9330 - ignore_accuracy: 0.2967 - val_loss: 0.2642 - val_acc: 0.9317 - val_ignore_accuracy: 0.2976\n",
      "Epoch 32/100\n",
      "1878/1878 [==============================] - 20s 10ms/step - loss: 0.2421 - acc: 0.9358 - ignore_accuracy: 0.3267 - val_loss: 0.2565 - val_acc: 0.9340 - val_ignore_accuracy: 0.3209\n",
      "Epoch 33/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.2330 - acc: 0.9392 - ignore_accuracy: 0.3619 - val_loss: 0.2460 - val_acc: 0.9388 - val_ignore_accuracy: 0.3712\n",
      "Epoch 34/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.2224 - acc: 0.9435 - ignore_accuracy: 0.4073 - val_loss: 0.2379 - val_acc: 0.9415 - val_ignore_accuracy: 0.3992\n",
      "Epoch 35/100\n",
      "1878/1878 [==============================] - 23s 12ms/step - loss: 0.2104 - acc: 0.9476 - ignore_accuracy: 0.4516 - val_loss: 0.2238 - val_acc: 0.9449 - val_ignore_accuracy: 0.4353\n",
      "Epoch 36/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.1973 - acc: 0.9514 - ignore_accuracy: 0.4913 - val_loss: 0.2111 - val_acc: 0.9499 - val_ignore_accuracy: 0.4870\n",
      "Epoch 37/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.1836 - acc: 0.9557 - ignore_accuracy: 0.5360 - val_loss: 0.1950 - val_acc: 0.9539 - val_ignore_accuracy: 0.5276\n",
      "Epoch 38/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.1695 - acc: 0.9596 - ignore_accuracy: 0.5767 - val_loss: 0.1853 - val_acc: 0.9553 - val_ignore_accuracy: 0.5441\n",
      "Epoch 39/100\n",
      "1878/1878 [==============================] - 25s 13ms/step - loss: 0.1553 - acc: 0.9630 - ignore_accuracy: 0.6119 - val_loss: 0.1695 - val_acc: 0.9592 - val_ignore_accuracy: 0.5829\n",
      "Epoch 40/100\n",
      "1878/1878 [==============================] - 20s 10ms/step - loss: 0.1413 - acc: 0.9664 - ignore_accuracy: 0.6470 - val_loss: 0.1558 - val_acc: 0.9623 - val_ignore_accuracy: 0.6140\n",
      "Epoch 41/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.1284 - acc: 0.9691 - ignore_accuracy: 0.6760 - val_loss: 0.1438 - val_acc: 0.9657 - val_ignore_accuracy: 0.6480\n",
      "Epoch 42/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.1170 - acc: 0.9721 - ignore_accuracy: 0.7066 - val_loss: 0.1336 - val_acc: 0.9677 - val_ignore_accuracy: 0.6692\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.1064 - acc: 0.9747 - ignore_accuracy: 0.7338 - val_loss: 0.1251 - val_acc: 0.9701 - val_ignore_accuracy: 0.6944\n",
      "Epoch 44/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.0968 - acc: 0.9776 - ignore_accuracy: 0.7643 - val_loss: 0.1171 - val_acc: 0.9723 - val_ignore_accuracy: 0.7166\n",
      "Epoch 45/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.0882 - acc: 0.9801 - ignore_accuracy: 0.7901 - val_loss: 0.1089 - val_acc: 0.9744 - val_ignore_accuracy: 0.7366\n",
      "Epoch 46/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.0802 - acc: 0.9822 - ignore_accuracy: 0.8130 - val_loss: 0.1024 - val_acc: 0.9761 - val_ignore_accuracy: 0.7551\n",
      "Epoch 47/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.0728 - acc: 0.9844 - ignore_accuracy: 0.8359 - val_loss: 0.0963 - val_acc: 0.9776 - val_ignore_accuracy: 0.7710\n",
      "Epoch 48/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.0662 - acc: 0.9863 - ignore_accuracy: 0.8553 - val_loss: 0.0918 - val_acc: 0.9788 - val_ignore_accuracy: 0.7842\n",
      "Epoch 49/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.0602 - acc: 0.9879 - ignore_accuracy: 0.8721 - val_loss: 0.0858 - val_acc: 0.9805 - val_ignore_accuracy: 0.8009\n",
      "Epoch 50/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.0547 - acc: 0.9893 - ignore_accuracy: 0.8872 - val_loss: 0.0818 - val_acc: 0.9817 - val_ignore_accuracy: 0.8136\n",
      "Epoch 51/100\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.0501 - acc: 0.9904 - ignore_accuracy: 0.8987 - val_loss: 0.0781 - val_acc: 0.9829 - val_ignore_accuracy: 0.8254\n",
      "Epoch 52/100\n",
      "1878/1878 [==============================] - 22s 12ms/step - loss: 0.0455 - acc: 0.9915 - ignore_accuracy: 0.9109 - val_loss: 0.0742 - val_acc: 0.9833 - val_ignore_accuracy: 0.8301\n",
      "Epoch 53/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0416 - acc: 0.9924 - ignore_accuracy: 0.9198 - val_loss: 0.0713 - val_acc: 0.9841 - val_ignore_accuracy: 0.8392\n",
      "Epoch 54/100\n",
      "1878/1878 [==============================] - 18s 9ms/step - loss: 0.0381 - acc: 0.9931 - ignore_accuracy: 0.9270 - val_loss: 0.0683 - val_acc: 0.9849 - val_ignore_accuracy: 0.8465\n",
      "Epoch 55/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0349 - acc: 0.9937 - ignore_accuracy: 0.9334 - val_loss: 0.0660 - val_acc: 0.9853 - val_ignore_accuracy: 0.8506\n",
      "Epoch 56/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0321 - acc: 0.9943 - ignore_accuracy: 0.9396 - val_loss: 0.0641 - val_acc: 0.9857 - val_ignore_accuracy: 0.8553\n",
      "Epoch 57/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0296 - acc: 0.9948 - ignore_accuracy: 0.9451 - val_loss: 0.0633 - val_acc: 0.9860 - val_ignore_accuracy: 0.8584\n",
      "Epoch 58/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0275 - acc: 0.9951 - ignore_accuracy: 0.9485 - val_loss: 0.0612 - val_acc: 0.9863 - val_ignore_accuracy: 0.8617\n",
      "Epoch 59/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0255 - acc: 0.9955 - ignore_accuracy: 0.9520 - val_loss: 0.0599 - val_acc: 0.9862 - val_ignore_accuracy: 0.8620\n",
      "Epoch 60/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0238 - acc: 0.9957 - ignore_accuracy: 0.9553 - val_loss: 0.0585 - val_acc: 0.9868 - val_ignore_accuracy: 0.8663\n",
      "Epoch 61/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0223 - acc: 0.9961 - ignore_accuracy: 0.9586 - val_loss: 0.0563 - val_acc: 0.9871 - val_ignore_accuracy: 0.8696\n",
      "Epoch 62/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0208 - acc: 0.9963 - ignore_accuracy: 0.9611 - val_loss: 0.0558 - val_acc: 0.9871 - val_ignore_accuracy: 0.8698\n",
      "Epoch 63/100\n",
      "1878/1878 [==============================] - 18s 9ms/step - loss: 0.0195 - acc: 0.9965 - ignore_accuracy: 0.9633 - val_loss: 0.0546 - val_acc: 0.9874 - val_ignore_accuracy: 0.8722\n",
      "Epoch 64/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0183 - acc: 0.9968 - ignore_accuracy: 0.9659 - val_loss: 0.0545 - val_acc: 0.9873 - val_ignore_accuracy: 0.8723\n",
      "Epoch 65/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.0172 - acc: 0.9969 - ignore_accuracy: 0.9678 - val_loss: 0.0532 - val_acc: 0.9876 - val_ignore_accuracy: 0.8743\n",
      "Epoch 66/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0162 - acc: 0.9971 - ignore_accuracy: 0.9695 - val_loss: 0.0531 - val_acc: 0.9875 - val_ignore_accuracy: 0.8738\n",
      "Epoch 67/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0153 - acc: 0.9973 - ignore_accuracy: 0.9711 - val_loss: 0.0528 - val_acc: 0.9877 - val_ignore_accuracy: 0.8754\n",
      "Epoch 68/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0145 - acc: 0.9974 - ignore_accuracy: 0.9721 - val_loss: 0.0519 - val_acc: 0.9877 - val_ignore_accuracy: 0.8755\n",
      "Epoch 69/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0137 - acc: 0.9977 - ignore_accuracy: 0.9756 - val_loss: 0.0517 - val_acc: 0.9880 - val_ignore_accuracy: 0.8789\n",
      "Epoch 70/100\n",
      "1878/1878 [==============================] - 17s 9ms/step - loss: 0.0131 - acc: 0.9978 - ignore_accuracy: 0.9771 - val_loss: 0.0509 - val_acc: 0.9880 - val_ignore_accuracy: 0.8788\n",
      "Epoch 71/100\n",
      "1878/1878 [==============================] - 18s 9ms/step - loss: 0.0125 - acc: 0.9978 - ignore_accuracy: 0.9774 - val_loss: 0.0497 - val_acc: 0.9883 - val_ignore_accuracy: 0.8813\n",
      "Epoch 72/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0119 - acc: 0.9979 - ignore_accuracy: 0.9783 - val_loss: 0.0498 - val_acc: 0.9882 - val_ignore_accuracy: 0.8801\n",
      "Epoch 73/100\n",
      "1878/1878 [==============================] - 25s 13ms/step - loss: 0.0112 - acc: 0.9981 - ignore_accuracy: 0.9798 - val_loss: 0.0501 - val_acc: 0.9882 - val_ignore_accuracy: 0.8802\n",
      "Epoch 74/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0107 - acc: 0.9981 - ignore_accuracy: 0.9803 - val_loss: 0.0504 - val_acc: 0.9881 - val_ignore_accuracy: 0.8792\n",
      "Epoch 75/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.0102 - acc: 0.9982 - ignore_accuracy: 0.9812 - val_loss: 0.0495 - val_acc: 0.9882 - val_ignore_accuracy: 0.8806\n",
      "Epoch 76/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0098 - acc: 0.9983 - ignore_accuracy: 0.9822 - val_loss: 0.0495 - val_acc: 0.9883 - val_ignore_accuracy: 0.8815\n",
      "Epoch 77/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0093 - acc: 0.9984 - ignore_accuracy: 0.9828 - val_loss: 0.0496 - val_acc: 0.9883 - val_ignore_accuracy: 0.8817\n",
      "Epoch 78/100\n",
      "1878/1878 [==============================] - 24s 13ms/step - loss: 0.0089 - acc: 0.9984 - ignore_accuracy: 0.9836 - val_loss: 0.0498 - val_acc: 0.9883 - val_ignore_accuracy: 0.8818\n",
      "Epoch 79/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.0085 - acc: 0.9985 - ignore_accuracy: 0.9843 - val_loss: 0.0497 - val_acc: 0.9883 - val_ignore_accuracy: 0.8819\n",
      "Epoch 80/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.0082 - acc: 0.9986 - ignore_accuracy: 0.9851 - val_loss: 0.0492 - val_acc: 0.9885 - val_ignore_accuracy: 0.8833\n",
      "Epoch 81/100\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.0078 - acc: 0.9987 - ignore_accuracy: 0.9858 - val_loss: 0.0501 - val_acc: 0.9883 - val_ignore_accuracy: 0.8820\n",
      "Epoch 82/100\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.0076 - acc: 0.9987 - ignore_accuracy: 0.9862 - val_loss: 0.0505 - val_acc: 0.9883 - val_ignore_accuracy: 0.8812\n",
      "Epoch 83/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0073 - acc: 0.9987 - ignore_accuracy: 0.9867 - val_loss: 0.0498 - val_acc: 0.9883 - val_ignore_accuracy: 0.8812\n",
      "Epoch 84/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0070 - acc: 0.9988 - ignore_accuracy: 0.9871 - val_loss: 0.0492 - val_acc: 0.9885 - val_ignore_accuracy: 0.8831\n",
      "Epoch 85/100\n",
      "1878/1878 [==============================] - 21s 11ms/step - loss: 0.0068 - acc: 0.9988 - ignore_accuracy: 0.9875 - val_loss: 0.0498 - val_acc: 0.9884 - val_ignore_accuracy: 0.8823\n",
      "Epoch 86/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.0065 - acc: 0.9989 - ignore_accuracy: 0.9880 - val_loss: 0.0502 - val_acc: 0.9884 - val_ignore_accuracy: 0.8827\n",
      "Epoch 87/100\n",
      "1878/1878 [==============================] - 20s 11ms/step - loss: 0.0063 - acc: 0.9989 - ignore_accuracy: 0.9884 - val_loss: 0.0500 - val_acc: 0.9884 - val_ignore_accuracy: 0.8825\n",
      "Epoch 88/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.0060 - acc: 0.9990 - ignore_accuracy: 0.9891 - val_loss: 0.0498 - val_acc: 0.9884 - val_ignore_accuracy: 0.8825\n",
      "Epoch 89/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.0058 - acc: 0.9990 - ignore_accuracy: 0.9896 - val_loss: 0.0501 - val_acc: 0.9884 - val_ignore_accuracy: 0.8822\n",
      "Epoch 90/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0056 - acc: 0.9990 - ignore_accuracy: 0.9897 - val_loss: 0.0508 - val_acc: 0.9884 - val_ignore_accuracy: 0.8826\n",
      "Epoch 91/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.0054 - acc: 0.9991 - ignore_accuracy: 0.9900 - val_loss: 0.0505 - val_acc: 0.9884 - val_ignore_accuracy: 0.8816\n",
      "Epoch 92/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0053 - acc: 0.9991 - ignore_accuracy: 0.9904 - val_loss: 0.0506 - val_acc: 0.9884 - val_ignore_accuracy: 0.8825\n",
      "Epoch 93/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0051 - acc: 0.9991 - ignore_accuracy: 0.9906 - val_loss: 0.0504 - val_acc: 0.9885 - val_ignore_accuracy: 0.8831\n",
      "Epoch 94/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.0049 - acc: 0.9992 - ignore_accuracy: 0.9912 - val_loss: 0.0508 - val_acc: 0.9883 - val_ignore_accuracy: 0.8816\n",
      "Epoch 95/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.0047 - acc: 0.9992 - ignore_accuracy: 0.9916 - val_loss: 0.0511 - val_acc: 0.9884 - val_ignore_accuracy: 0.8826\n",
      "Epoch 96/100\n",
      "1878/1878 [==============================] - 24s 13ms/step - loss: 0.0046 - acc: 0.9992 - ignore_accuracy: 0.9919 - val_loss: 0.0514 - val_acc: 0.9885 - val_ignore_accuracy: 0.8829\n",
      "Epoch 97/100\n",
      "1878/1878 [==============================] - 19s 10ms/step - loss: 0.0045 - acc: 0.9993 - ignore_accuracy: 0.9922 - val_loss: 0.0512 - val_acc: 0.9885 - val_ignore_accuracy: 0.8831\n",
      "Epoch 98/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0043 - acc: 0.9993 - ignore_accuracy: 0.9922 - val_loss: 0.0510 - val_acc: 0.9885 - val_ignore_accuracy: 0.8828\n",
      "Epoch 99/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0042 - acc: 0.9993 - ignore_accuracy: 0.9927 - val_loss: 0.0512 - val_acc: 0.9886 - val_ignore_accuracy: 0.8840\n",
      "Epoch 100/100\n",
      "1878/1878 [==============================] - 18s 10ms/step - loss: 0.0040 - acc: 0.9994 - ignore_accuracy: 0.9933 - val_loss: 0.0519 - val_acc: 0.9885 - val_ignore_accuracy: 0.8829\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=100, validation_split=0.2) #,\n",
    "#                    callbacks=[EarlyStopping(monitor='val_ignore_accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['VBG', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_samples_X)\n",
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see the great difference in accuracy when we ignore the padding symbol. Also, note that the results reported by the original post https://nlpforhackers.io/lstm-pos-tagger-keras/ are better, probably because they used two LSTM chains: one going forwards, and another going backwards. This is what is called a **bidirectional recurrent network**, and it usually reports better results because each prediction is based on the context on the left and the right of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
