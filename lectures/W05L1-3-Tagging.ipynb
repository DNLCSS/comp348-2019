{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W05L1-3-Tagging.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "AWyCmzO622th",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building a Part of Speech Tagger with Keras\n",
        "\n",
        "This notebook is based on nlpforhackers' post: https://nlpforhackers.io/lstm-pos-tagger-keras/. We will look at how to build a part of speech tagger using a LSTM layer.\n",
        "\n",
        "We will use NLTK's treebank corpus. This corpus has, among other kinds of information, annotations about the parts of speech of the words."
      ]
    },
    {
      "metadata": {
        "id": "dugp1ZSX22tm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "7342810e-a878-4f18-9ced-57689022b159"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        " \n",
        "print(tagged_sentences[0])\n",
        "print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "Tagged sentences:  3914\n",
            "Tagged words: 100676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "aKG3Wi1k22tx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As always, before training a model, we need to split the data in training and testing data:"
      ]
    },
    {
      "metadata": {
        "id": "B0BTKbIe22tz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(1234)\n",
        "tagged_sentences = list(tagged_sentences) # we convert the data to a list\n",
        "random.shuffle(tagged_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sEkj81QG22t4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "threshold = int(len(tagged_sentences)*.6)\n",
        "train = tagged_sentences[:threshold]\n",
        "test = tagged_sentences[threshold:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "42eTQ3kQ22t9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let’s restructure the data a bit. Let’s separate the words from the tags."
      ]
    },
    {
      "metadata": {
        "id": "J-DZLM5e22t_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        " \n",
        "train_sentences, train_tags =[], [] \n",
        "for tagged_sentence in train:\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    train_sentences.append(np.array(sentence))\n",
        "    train_tags.append(np.array(tags))\n",
        "    \n",
        "test_sentences, test_tags =[], [] \n",
        "for tagged_sentence in test:\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    test_sentences.append(np.array(sentence))\n",
        "    test_tags.append(np.array(tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aKlHJ21O22uE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "92256bab-6e33-4dd7-bd17-8c57170c417a"
      },
      "cell_type": "code",
      "source": [
        "print(train_sentences[5])\n",
        "print(train_tags[5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['They' 'are' 'keeping' 'a' 'close' 'watch' 'on' 'the' 'yield' 'on' 'the'\n",
            " 'S&P' '500' '.']\n",
            "['PRP' 'VBP' 'VBG' 'DT' 'JJ' 'NN' 'IN' 'DT' 'NN' 'IN' 'DT' 'NNP' 'CD' '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "TO6Fo_ez22uK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now convert the words and tags to indices. We will reserve index 0 for padding, and index 1 for words that are out of the vocabulary (**OOV - Out of Vocabulary**)."
      ]
    },
    {
      "metadata": {
        "id": "Dpy45jUr22uM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words, tags = set([]), set([])\n",
        " \n",
        "for s in train_sentences:\n",
        "    for w in s:\n",
        "        words.add(w.lower())\n",
        " \n",
        "for ts in train_tags:\n",
        "    for t in ts:\n",
        "        tags.add(t)\n",
        " \n",
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # The special value used for padding\n",
        "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
        " \n",
        "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0  # The special value used to padding\n",
        "tag2index['-OOV-'] = 1  # There may also be unknown tags in the test data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "tz7jJt1R22uQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "2dadfa40-1356-4eb1-e0db-c32dfb243863"
      },
      "cell_type": "code",
      "source": [
        "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        " \n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    s_int = []\n",
        "    for t in s:\n",
        "        try:\n",
        "            s_int.append(tag2index[t])\n",
        "        except KeyError:\n",
        "            s_int.append(tag2index['-OOV-'])\n",
        "            \n",
        "    test_tags_y.append(s_int)\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[608, 1472, 7273, 6180, 8361, 4026, 7926, 2929]\n",
            "[1447, 6569, 844, 8084, 5333, 1, 2200, 6384, 2627, 6771, 3773, 6384, 1, 181, 6908, 8361, 6508, 7064, 6384, 1760, 3003, 1, 2929]\n",
            "[27, 25, 30, 23, 36, 22, 22, 41]\n",
            "[38, 23, 17, 20, 23, 17, 32, 16, 3, 38, 17, 16, 17, 32, 17, 36, 42, 17, 16, 32, 37, 17, 41]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "tyBmXkCw22uW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since Keras can only deal with fixed size sequences, we will pad all sequences to fit the longest sentence in the training data."
      ]
    },
    {
      "metadata": {
        "id": "e0ZdPDZA22uY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1896d494-811b-432f-9bb4-1827e09776c0"
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(MAX_LENGTH)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "S8fOCFoD22ud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1187
        },
        "outputId": "3b9cf831-5855-44d2-eb6a-212c05627a7f"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ 608 1472 7273 6180 8361 4026 7926 2929    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[1447 6569  844 8084 5333    1 2200 6384 2627 6771 3773 6384    1  181\n",
            " 6908 8361 6508 7064 6384 1760 3003    1 2929    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[27 25 30 23 36 22 22 41  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n",
            "[38 23 17 20 23 17 32 16  3 38 17 16 17 32 17 36 42 17 16 32 37 17 41  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "f0w8DzeQ22uj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now define the model. \n",
        "A simple approach to build a part-of-speech tagger in Keras is to stack a dense layer at the output of each RNN cell. This is achieved with `TimeDistributed`. Note  how the output of the model is now a list of sequences of vectors. Each vector will contain the probability of each tag. In particular:\n",
        "\n",
        "* We will introduce an embedding layer.\n",
        "* THe LSTM layer will have the parameter `return_sequences=True` so that we have access to the entire sequence.\n",
        "* The output of each LSTM cell will have a `Dense` layer. We can do this using the `TimeDistributed` layer."
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "f6buON2e22ul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "99940122-e876-4f72-d0b5-dc16cc027bba"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 271, 128)          1111424   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 271, 256)          394240    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 271, 47)           12079     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 1,517,743\n",
            "Trainable params: 1,517,743\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "UeE6WfGH22ur",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We also need to convert the labels into one-hot encoding. Since the input are sequences of labels we cannot use Keras' `to_categorical` so we define our own function:"
      ]
    },
    {
      "metadata": {
        "id": "bJTyy7hA22uu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xWr3YL_S22uy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "bee1cb7b-dc17-4807-d744-f6d6fe9864cc"
      },
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "print(cat_train_tags_y[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "XLqIilKl22u5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can finally train the model:"
      ]
    },
    {
      "metadata": {
        "id": "7pTh_pR922vA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1603
        },
        "outputId": "26018680-4e29-4345-beb3-e66a0c768dd1"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_split=0.2)\n",
        " "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 1878 samples, validate on 470 samples\n",
            "Epoch 1/40\n",
            "1878/1878 [==============================] - 12s 7ms/step - loss: 1.7536 - acc: 0.8438 - val_loss: 0.6165 - val_acc: 0.9036\n",
            "Epoch 2/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.7007 - acc: 0.8763 - val_loss: 0.4678 - val_acc: 0.9037\n",
            "Epoch 3/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.4189 - acc: 0.9056 - val_loss: 0.4092 - val_acc: 0.9043\n",
            "Epoch 4/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3900 - acc: 0.9061 - val_loss: 0.3853 - val_acc: 0.9040\n",
            "Epoch 5/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3726 - acc: 0.9062 - val_loss: 0.3740 - val_acc: 0.9041\n",
            "Epoch 6/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3576 - acc: 0.9062 - val_loss: 0.3672 - val_acc: 0.9041\n",
            "Epoch 7/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3452 - acc: 0.9065 - val_loss: 0.3512 - val_acc: 0.9056\n",
            "Epoch 8/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3332 - acc: 0.9083 - val_loss: 0.3409 - val_acc: 0.9087\n",
            "Epoch 9/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3249 - acc: 0.9108 - val_loss: 0.3390 - val_acc: 0.9101\n",
            "Epoch 10/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3317 - acc: 0.9136 - val_loss: 0.3490 - val_acc: 0.9130\n",
            "Epoch 11/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3250 - acc: 0.9131 - val_loss: 0.3213 - val_acc: 0.9126\n",
            "Epoch 12/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3124 - acc: 0.9150 - val_loss: 0.3152 - val_acc: 0.9138\n",
            "Epoch 13/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3054 - acc: 0.9153 - val_loss: 0.3095 - val_acc: 0.9145\n",
            "Epoch 14/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2996 - acc: 0.9165 - val_loss: 0.3061 - val_acc: 0.9170\n",
            "Epoch 15/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2947 - acc: 0.9187 - val_loss: 0.3021 - val_acc: 0.9182\n",
            "Epoch 16/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2906 - acc: 0.9197 - val_loss: 0.2990 - val_acc: 0.9188\n",
            "Epoch 17/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2873 - acc: 0.9202 - val_loss: 0.2966 - val_acc: 0.9192\n",
            "Epoch 18/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2847 - acc: 0.9205 - val_loss: 0.2946 - val_acc: 0.9194\n",
            "Epoch 19/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2824 - acc: 0.9208 - val_loss: 0.2931 - val_acc: 0.9198\n",
            "Epoch 20/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2805 - acc: 0.9216 - val_loss: 0.2913 - val_acc: 0.9206\n",
            "Epoch 21/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2785 - acc: 0.9220 - val_loss: 0.2896 - val_acc: 0.9209\n",
            "Epoch 22/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2767 - acc: 0.9222 - val_loss: 0.2893 - val_acc: 0.9211\n",
            "Epoch 23/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2747 - acc: 0.9225 - val_loss: 0.2876 - val_acc: 0.9215\n",
            "Epoch 24/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2725 - acc: 0.9228 - val_loss: 0.2862 - val_acc: 0.9218\n",
            "Epoch 25/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2701 - acc: 0.9232 - val_loss: 0.2852 - val_acc: 0.9228\n",
            "Epoch 26/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2673 - acc: 0.9243 - val_loss: 0.2815 - val_acc: 0.9240\n",
            "Epoch 27/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2639 - acc: 0.9264 - val_loss: 0.2787 - val_acc: 0.9257\n",
            "Epoch 28/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2598 - acc: 0.9289 - val_loss: 0.2759 - val_acc: 0.9270\n",
            "Epoch 29/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2545 - acc: 0.9327 - val_loss: 0.2697 - val_acc: 0.9313\n",
            "Epoch 30/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2481 - acc: 0.9375 - val_loss: 0.2632 - val_acc: 0.9367\n",
            "Epoch 31/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2395 - acc: 0.9423 - val_loss: 0.2559 - val_acc: 0.9406\n",
            "Epoch 32/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2295 - acc: 0.9454 - val_loss: 0.2427 - val_acc: 0.9443\n",
            "Epoch 33/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2169 - acc: 0.9488 - val_loss: 0.2300 - val_acc: 0.9463\n",
            "Epoch 34/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2037 - acc: 0.9521 - val_loss: 0.2152 - val_acc: 0.9492\n",
            "Epoch 35/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1893 - acc: 0.9551 - val_loss: 0.1996 - val_acc: 0.9527\n",
            "Epoch 36/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1741 - acc: 0.9588 - val_loss: 0.1862 - val_acc: 0.9550\n",
            "Epoch 37/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1598 - acc: 0.9620 - val_loss: 0.1735 - val_acc: 0.9583\n",
            "Epoch 38/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1468 - acc: 0.9652 - val_loss: 0.1617 - val_acc: 0.9614\n",
            "Epoch 39/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1346 - acc: 0.9687 - val_loss: 0.1518 - val_acc: 0.9639\n",
            "Epoch 40/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1233 - acc: 0.9717 - val_loss: 0.1413 - val_acc: 0.9670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "5zNVFn7W22vM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the results look too good, it's because they are too good! We need to ignore the classification of the padded symbols, but let's leave it aside for now. Below is an evaluation using the test data."
      ]
    },
    {
      "metadata": {
        "id": "8YVnb9mv22vP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "02611832-919f-46cc-9793-86c485c619bc"
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
        "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1566/1566 [==============================] - 11s 7ms/step\n",
            "acc: 96.76379513466496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "h197tLfl22vY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's find the PoS of two test sentences:"
      ]
    },
    {
      "metadata": {
        "id": "TJn9HsNN22va",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e8d5b956-5c89-4dbf-a6f0-8bdbc196d362"
      },
      "cell_type": "code",
      "source": [
        "test_samples = [\n",
        "    \"running is very important for me .\".split(),\n",
        "    \"I was running every day for a month .\".split()\n",
        "]\n",
        "print(test_samples)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "NPD1nbGV22vf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These test sentences need to be vectorised:"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "DUSoAyOS22vg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "ea16b827-59e0-43f8-f7e3-1b7dc078e1e5"
      },
      "cell_type": "code",
      "source": [
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "    test_samples_X.append(s_int)\n",
        " \n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "print(test_samples_X)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3754 5970 4765 4048 4260 3809 2929    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [5194 4604 3754 4107 3170 4260 7380 3109 2929    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "Y6rLuQb322vl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And the predictions are ..."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "r5Qgjrq622vn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "e5ca507c-6efe-4aff-81b6-f26f55333085"
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_samples_X)\n",
        "print(predictions, predictions.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[2.91282479e-02 1.88667346e-02 1.89538840e-02 ... 1.99475978e-02\n",
            "   1.79031510e-02 1.74484272e-02]\n",
            "  [4.02032435e-02 1.36810364e-02 1.61603559e-02 ... 1.65906642e-02\n",
            "   1.41995354e-02 1.28826033e-02]\n",
            "  [2.85787806e-02 8.16254970e-03 1.41121214e-02 ... 1.23855462e-02\n",
            "   1.44241638e-02 6.86419895e-03]\n",
            "  ...\n",
            "  [9.99904513e-01 5.17404248e-14 1.22004394e-11 ... 1.90587433e-12\n",
            "   2.80383762e-14 7.35508795e-14]\n",
            "  [9.99904513e-01 5.17822852e-14 1.22030224e-11 ... 1.90607794e-12\n",
            "   2.80099396e-14 7.35358700e-14]\n",
            "  [9.99904633e-01 5.18243793e-14 1.22056072e-11 ... 1.90628893e-12\n",
            "   2.79813743e-14 7.35205828e-14]]\n",
            "\n",
            " [[2.80413087e-02 1.51930703e-02 1.81705691e-02 ... 1.76218972e-02\n",
            "   1.74059626e-02 1.45811960e-02]\n",
            "  [3.12418342e-02 9.50537995e-03 1.50221204e-02 ... 1.29120313e-02\n",
            "   1.52053805e-02 8.47656094e-03]\n",
            "  [3.91318537e-02 7.02448050e-03 1.37202442e-02 ... 1.28371343e-02\n",
            "   1.30154779e-02 5.18636452e-03]\n",
            "  ...\n",
            "  [9.99900579e-01 7.95620351e-14 1.56737418e-11 ... 2.54195926e-12\n",
            "   2.53709831e-14 6.52491434e-14]\n",
            "  [9.99900341e-01 8.02236627e-14 1.57929034e-11 ... 2.56808420e-12\n",
            "   2.55599171e-14 6.50859236e-14]\n",
            "  [9.99899983e-01 8.08812178e-14 1.59118448e-11 ... 2.59423971e-12\n",
            "   2.57507011e-14 6.49258343e-14]]] (2, 271, 47)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "hYyJkHb_22vt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These predictions show, for each word, the probabilities assigned to each possible label. Let's choose the label with the highest probability (using numpy's `argmax`) and then convert from label index to label:"
      ]
    },
    {
      "metadata": {
        "id": "s1EEpd2K22vv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "P-vgNHpt22vz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "5b4378f9-3b0d-4379-a001-a7b62784bd9a"
      },
      "cell_type": "code",
      "source": [
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['CD', '-PAD-', ',', 'DT', 'IN', 'NNS', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['NNP', ',', 'NNP', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "xUAVFac_22v6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As expected, the padding symbol is over-used. To solve this problem we can define a custom accuracy metric that ignores the paddings. Remember that the index for the padding symbol is 0:"
      ]
    },
    {
      "metadata": {
        "id": "htLSFx1722v8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        " \n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "owboYAh022v_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now retrain using this new accuracy metric:"
      ]
    },
    {
      "metadata": {
        "id": "Adx9EbTB22wC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "f3e68b68-3642-4a7c-e4d7-a66d27f09c09"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 271, 128)          1111424   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 271, 256)          394240    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 271, 47)           12079     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 1,517,743\n",
            "Trainable params: 1,517,743\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "vglktjZL22wI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3655
        },
        "outputId": "d412e4e8-7f1b-41c3-ff60-babe36a72e6e"
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=100, validation_split=0.2) #,\n",
        "#                    callbacks=[EarlyStopping(monitor='val_ignore_accuracy')])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1878 samples, validate on 470 samples\n",
            "Epoch 1/100\n",
            "1878/1878 [==============================] - 10s 5ms/step - loss: 1.7742 - acc: 0.8438 - ignore_accuracy: 0.0110 - val_loss: 0.6140 - val_acc: 0.9036 - val_ignore_accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.6275 - acc: 0.8870 - ignore_accuracy: 0.0494 - val_loss: 0.4716 - val_acc: 0.9036 - val_ignore_accuracy: 0.0663\n",
            "Epoch 3/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.4263 - acc: 0.9055 - ignore_accuracy: 0.1279 - val_loss: 0.4114 - val_acc: 0.9040 - val_ignore_accuracy: 0.1249\n",
            "Epoch 4/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3923 - acc: 0.9059 - ignore_accuracy: 0.1154 - val_loss: 0.3885 - val_acc: 0.9039 - val_ignore_accuracy: 0.1071\n",
            "Epoch 5/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3714 - acc: 0.9061 - ignore_accuracy: 0.1194 - val_loss: 0.3821 - val_acc: 0.9039 - val_ignore_accuracy: 0.0921\n",
            "Epoch 6/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3562 - acc: 0.9063 - ignore_accuracy: 0.1150 - val_loss: 0.3603 - val_acc: 0.9062 - val_ignore_accuracy: 0.1356\n",
            "Epoch 7/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3448 - acc: 0.9077 - ignore_accuracy: 0.1242 - val_loss: 0.3555 - val_acc: 0.9057 - val_ignore_accuracy: 0.1292\n",
            "Epoch 8/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3347 - acc: 0.9095 - ignore_accuracy: 0.1297 - val_loss: 0.3445 - val_acc: 0.9069 - val_ignore_accuracy: 0.1313\n",
            "Epoch 9/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3265 - acc: 0.9099 - ignore_accuracy: 0.1261 - val_loss: 0.3337 - val_acc: 0.9101 - val_ignore_accuracy: 0.1367\n",
            "Epoch 10/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3197 - acc: 0.9123 - ignore_accuracy: 0.1310 - val_loss: 0.3302 - val_acc: 0.9120 - val_ignore_accuracy: 0.1389\n",
            "Epoch 11/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3138 - acc: 0.9141 - ignore_accuracy: 0.1333 - val_loss: 0.3331 - val_acc: 0.9126 - val_ignore_accuracy: 0.1388\n",
            "Epoch 12/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3081 - acc: 0.9150 - ignore_accuracy: 0.1352 - val_loss: 0.3328 - val_acc: 0.9140 - val_ignore_accuracy: 0.1477\n",
            "Epoch 13/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3258 - acc: 0.9155 - ignore_accuracy: 0.1389 - val_loss: 0.3129 - val_acc: 0.9148 - val_ignore_accuracy: 0.1482\n",
            "Epoch 14/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3027 - acc: 0.9160 - ignore_accuracy: 0.1416 - val_loss: 0.3074 - val_acc: 0.9170 - val_ignore_accuracy: 0.1648\n",
            "Epoch 15/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2977 - acc: 0.9183 - ignore_accuracy: 0.1614 - val_loss: 0.3037 - val_acc: 0.9179 - val_ignore_accuracy: 0.1723\n",
            "Epoch 16/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2933 - acc: 0.9191 - ignore_accuracy: 0.1669 - val_loss: 0.3005 - val_acc: 0.9184 - val_ignore_accuracy: 0.1736\n",
            "Epoch 17/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2898 - acc: 0.9198 - ignore_accuracy: 0.1698 - val_loss: 0.2983 - val_acc: 0.9189 - val_ignore_accuracy: 0.1750\n",
            "Epoch 18/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2870 - acc: 0.9202 - ignore_accuracy: 0.1709 - val_loss: 0.2964 - val_acc: 0.9192 - val_ignore_accuracy: 0.1754\n",
            "Epoch 19/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2846 - acc: 0.9205 - ignore_accuracy: 0.1722 - val_loss: 0.2946 - val_acc: 0.9195 - val_ignore_accuracy: 0.1762\n",
            "Epoch 20/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2825 - acc: 0.9208 - ignore_accuracy: 0.1732 - val_loss: 0.2933 - val_acc: 0.9197 - val_ignore_accuracy: 0.1772\n",
            "Epoch 21/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2805 - acc: 0.9211 - ignore_accuracy: 0.1753 - val_loss: 0.2920 - val_acc: 0.9199 - val_ignore_accuracy: 0.1780\n",
            "Epoch 22/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2786 - acc: 0.9213 - ignore_accuracy: 0.1757 - val_loss: 0.2903 - val_acc: 0.9202 - val_ignore_accuracy: 0.1801\n",
            "Epoch 23/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2767 - acc: 0.9218 - ignore_accuracy: 0.1808 - val_loss: 0.2908 - val_acc: 0.9204 - val_ignore_accuracy: 0.1818\n",
            "Epoch 24/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2746 - acc: 0.9225 - ignore_accuracy: 0.1880 - val_loss: 0.2881 - val_acc: 0.9214 - val_ignore_accuracy: 0.1920\n",
            "Epoch 25/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2724 - acc: 0.9239 - ignore_accuracy: 0.2026 - val_loss: 0.2862 - val_acc: 0.9228 - val_ignore_accuracy: 0.2056\n",
            "Epoch 26/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2699 - acc: 0.9248 - ignore_accuracy: 0.2110 - val_loss: 0.2839 - val_acc: 0.9238 - val_ignore_accuracy: 0.2155\n",
            "Epoch 27/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2670 - acc: 0.9266 - ignore_accuracy: 0.2298 - val_loss: 0.2812 - val_acc: 0.9269 - val_ignore_accuracy: 0.2475\n",
            "Epoch 28/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2633 - acc: 0.9290 - ignore_accuracy: 0.2560 - val_loss: 0.2790 - val_acc: 0.9279 - val_ignore_accuracy: 0.2582\n",
            "Epoch 29/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2588 - acc: 0.9315 - ignore_accuracy: 0.2815 - val_loss: 0.2742 - val_acc: 0.9316 - val_ignore_accuracy: 0.2969\n",
            "Epoch 30/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2531 - acc: 0.9349 - ignore_accuracy: 0.3176 - val_loss: 0.2679 - val_acc: 0.9342 - val_ignore_accuracy: 0.3233\n",
            "Epoch 31/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2460 - acc: 0.9381 - ignore_accuracy: 0.3519 - val_loss: 0.2610 - val_acc: 0.9395 - val_ignore_accuracy: 0.3778\n",
            "Epoch 32/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2372 - acc: 0.9428 - ignore_accuracy: 0.4017 - val_loss: 0.2540 - val_acc: 0.9400 - val_ignore_accuracy: 0.3835\n",
            "Epoch 33/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2262 - acc: 0.9469 - ignore_accuracy: 0.4443 - val_loss: 0.2382 - val_acc: 0.9469 - val_ignore_accuracy: 0.4551\n",
            "Epoch 34/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2132 - acc: 0.9512 - ignore_accuracy: 0.4892 - val_loss: 0.2284 - val_acc: 0.9483 - val_ignore_accuracy: 0.4697\n",
            "Epoch 35/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1992 - acc: 0.9547 - ignore_accuracy: 0.5259 - val_loss: 0.2120 - val_acc: 0.9523 - val_ignore_accuracy: 0.5115\n",
            "Epoch 36/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1846 - acc: 0.9578 - ignore_accuracy: 0.5582 - val_loss: 0.1961 - val_acc: 0.9557 - val_ignore_accuracy: 0.5459\n",
            "Epoch 37/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1704 - acc: 0.9607 - ignore_accuracy: 0.5888 - val_loss: 0.1844 - val_acc: 0.9573 - val_ignore_accuracy: 0.5635\n",
            "Epoch 38/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1572 - acc: 0.9634 - ignore_accuracy: 0.6174 - val_loss: 0.1739 - val_acc: 0.9589 - val_ignore_accuracy: 0.5798\n",
            "Epoch 39/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1452 - acc: 0.9663 - ignore_accuracy: 0.6470 - val_loss: 0.1599 - val_acc: 0.9629 - val_ignore_accuracy: 0.6213\n",
            "Epoch 40/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1335 - acc: 0.9695 - ignore_accuracy: 0.6815 - val_loss: 0.1492 - val_acc: 0.9654 - val_ignore_accuracy: 0.6476\n",
            "Epoch 41/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1225 - acc: 0.9726 - ignore_accuracy: 0.7135 - val_loss: 0.1396 - val_acc: 0.9675 - val_ignore_accuracy: 0.6692\n",
            "Epoch 42/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1124 - acc: 0.9753 - ignore_accuracy: 0.7417 - val_loss: 0.1303 - val_acc: 0.9708 - val_ignore_accuracy: 0.7026\n",
            "Epoch 43/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1027 - acc: 0.9782 - ignore_accuracy: 0.7716 - val_loss: 0.1206 - val_acc: 0.9734 - val_ignore_accuracy: 0.7289\n",
            "Epoch 44/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0938 - acc: 0.9805 - ignore_accuracy: 0.7964 - val_loss: 0.1128 - val_acc: 0.9752 - val_ignore_accuracy: 0.7469\n",
            "Epoch 45/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0854 - acc: 0.9826 - ignore_accuracy: 0.8188 - val_loss: 0.1056 - val_acc: 0.9772 - val_ignore_accuracy: 0.7666\n",
            "Epoch 46/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0779 - acc: 0.9845 - ignore_accuracy: 0.8381 - val_loss: 0.0995 - val_acc: 0.9783 - val_ignore_accuracy: 0.7787\n",
            "Epoch 47/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0711 - acc: 0.9862 - ignore_accuracy: 0.8559 - val_loss: 0.0927 - val_acc: 0.9802 - val_ignore_accuracy: 0.7972\n",
            "Epoch 48/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0647 - acc: 0.9879 - ignore_accuracy: 0.8724 - val_loss: 0.0876 - val_acc: 0.9815 - val_ignore_accuracy: 0.8103\n",
            "Epoch 49/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0588 - acc: 0.9892 - ignore_accuracy: 0.8864 - val_loss: 0.0827 - val_acc: 0.9825 - val_ignore_accuracy: 0.8205\n",
            "Epoch 50/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0536 - acc: 0.9904 - ignore_accuracy: 0.8991 - val_loss: 0.0785 - val_acc: 0.9835 - val_ignore_accuracy: 0.8309\n",
            "Epoch 51/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0491 - acc: 0.9913 - ignore_accuracy: 0.9085 - val_loss: 0.0745 - val_acc: 0.9843 - val_ignore_accuracy: 0.8381\n",
            "Epoch 52/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0450 - acc: 0.9921 - ignore_accuracy: 0.9165 - val_loss: 0.0711 - val_acc: 0.9849 - val_ignore_accuracy: 0.8444\n",
            "Epoch 53/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0414 - acc: 0.9927 - ignore_accuracy: 0.9229 - val_loss: 0.0682 - val_acc: 0.9854 - val_ignore_accuracy: 0.8502\n",
            "Epoch 54/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0382 - acc: 0.9932 - ignore_accuracy: 0.9284 - val_loss: 0.0658 - val_acc: 0.9858 - val_ignore_accuracy: 0.8541\n",
            "Epoch 55/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0354 - acc: 0.9937 - ignore_accuracy: 0.9338 - val_loss: 0.0637 - val_acc: 0.9861 - val_ignore_accuracy: 0.8572\n",
            "Epoch 56/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0329 - acc: 0.9941 - ignore_accuracy: 0.9385 - val_loss: 0.0617 - val_acc: 0.9865 - val_ignore_accuracy: 0.8605\n",
            "Epoch 57/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0307 - acc: 0.9945 - ignore_accuracy: 0.9419 - val_loss: 0.0598 - val_acc: 0.9867 - val_ignore_accuracy: 0.8635\n",
            "Epoch 58/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0286 - acc: 0.9949 - ignore_accuracy: 0.9457 - val_loss: 0.0584 - val_acc: 0.9871 - val_ignore_accuracy: 0.8675\n",
            "Epoch 59/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0268 - acc: 0.9952 - ignore_accuracy: 0.9491 - val_loss: 0.0569 - val_acc: 0.9874 - val_ignore_accuracy: 0.8700\n",
            "Epoch 60/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0252 - acc: 0.9955 - ignore_accuracy: 0.9524 - val_loss: 0.0558 - val_acc: 0.9874 - val_ignore_accuracy: 0.8706\n",
            "Epoch 61/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0237 - acc: 0.9958 - ignore_accuracy: 0.9561 - val_loss: 0.0549 - val_acc: 0.9876 - val_ignore_accuracy: 0.8728\n",
            "Epoch 62/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0224 - acc: 0.9961 - ignore_accuracy: 0.9590 - val_loss: 0.0537 - val_acc: 0.9879 - val_ignore_accuracy: 0.8756\n",
            "Epoch 63/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0212 - acc: 0.9963 - ignore_accuracy: 0.9613 - val_loss: 0.0530 - val_acc: 0.9879 - val_ignore_accuracy: 0.8759\n",
            "Epoch 64/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0201 - acc: 0.9965 - ignore_accuracy: 0.9627 - val_loss: 0.0520 - val_acc: 0.9880 - val_ignore_accuracy: 0.8760\n",
            "Epoch 65/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0191 - acc: 0.9966 - ignore_accuracy: 0.9644 - val_loss: 0.0515 - val_acc: 0.9882 - val_ignore_accuracy: 0.8781\n",
            "Epoch 66/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0182 - acc: 0.9967 - ignore_accuracy: 0.9655 - val_loss: 0.0509 - val_acc: 0.9882 - val_ignore_accuracy: 0.8789\n",
            "Epoch 67/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0173 - acc: 0.9968 - ignore_accuracy: 0.9664 - val_loss: 0.0503 - val_acc: 0.9882 - val_ignore_accuracy: 0.8787\n",
            "Epoch 68/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0165 - acc: 0.9969 - ignore_accuracy: 0.9675 - val_loss: 0.0498 - val_acc: 0.9883 - val_ignore_accuracy: 0.8796\n",
            "Epoch 69/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0157 - acc: 0.9971 - ignore_accuracy: 0.9691 - val_loss: 0.0496 - val_acc: 0.9883 - val_ignore_accuracy: 0.8800\n",
            "Epoch 70/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0150 - acc: 0.9972 - ignore_accuracy: 0.9700 - val_loss: 0.0491 - val_acc: 0.9883 - val_ignore_accuracy: 0.8794\n",
            "Epoch 71/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0144 - acc: 0.9972 - ignore_accuracy: 0.9705 - val_loss: 0.0485 - val_acc: 0.9885 - val_ignore_accuracy: 0.8815\n",
            "Epoch 72/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0138 - acc: 0.9973 - ignore_accuracy: 0.9714 - val_loss: 0.0484 - val_acc: 0.9885 - val_ignore_accuracy: 0.8818\n",
            "Epoch 73/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0133 - acc: 0.9976 - ignore_accuracy: 0.9749 - val_loss: 0.0482 - val_acc: 0.9887 - val_ignore_accuracy: 0.8836\n",
            "Epoch 74/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0127 - acc: 0.9978 - ignore_accuracy: 0.9765 - val_loss: 0.0475 - val_acc: 0.9888 - val_ignore_accuracy: 0.8845\n",
            "Epoch 75/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0122 - acc: 0.9979 - ignore_accuracy: 0.9774 - val_loss: 0.0477 - val_acc: 0.9888 - val_ignore_accuracy: 0.8846\n",
            "Epoch 76/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0118 - acc: 0.9979 - ignore_accuracy: 0.9774 - val_loss: 0.0472 - val_acc: 0.9889 - val_ignore_accuracy: 0.8859\n",
            "Epoch 77/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0113 - acc: 0.9980 - ignore_accuracy: 0.9784 - val_loss: 0.0468 - val_acc: 0.9889 - val_ignore_accuracy: 0.8860\n",
            "Epoch 78/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0109 - acc: 0.9980 - ignore_accuracy: 0.9793 - val_loss: 0.0469 - val_acc: 0.9889 - val_ignore_accuracy: 0.8859\n",
            "Epoch 79/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0105 - acc: 0.9981 - ignore_accuracy: 0.9800 - val_loss: 0.0468 - val_acc: 0.9890 - val_ignore_accuracy: 0.8869\n",
            "Epoch 80/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0101 - acc: 0.9981 - ignore_accuracy: 0.9803 - val_loss: 0.0464 - val_acc: 0.9890 - val_ignore_accuracy: 0.8864\n",
            "Epoch 81/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0098 - acc: 0.9982 - ignore_accuracy: 0.9810 - val_loss: 0.0465 - val_acc: 0.9890 - val_ignore_accuracy: 0.8861\n",
            "Epoch 82/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0094 - acc: 0.9983 - ignore_accuracy: 0.9822 - val_loss: 0.0463 - val_acc: 0.9890 - val_ignore_accuracy: 0.8862\n",
            "Epoch 83/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0091 - acc: 0.9983 - ignore_accuracy: 0.9824 - val_loss: 0.0464 - val_acc: 0.9890 - val_ignore_accuracy: 0.8862\n",
            "Epoch 84/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0088 - acc: 0.9984 - ignore_accuracy: 0.9833 - val_loss: 0.0464 - val_acc: 0.9890 - val_ignore_accuracy: 0.8872\n",
            "Epoch 85/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0085 - acc: 0.9985 - ignore_accuracy: 0.9837 - val_loss: 0.0463 - val_acc: 0.9890 - val_ignore_accuracy: 0.8864\n",
            "Epoch 86/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0082 - acc: 0.9985 - ignore_accuracy: 0.9843 - val_loss: 0.0467 - val_acc: 0.9890 - val_ignore_accuracy: 0.8865\n",
            "Epoch 87/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0079 - acc: 0.9985 - ignore_accuracy: 0.9847 - val_loss: 0.0464 - val_acc: 0.9890 - val_ignore_accuracy: 0.8863\n",
            "Epoch 88/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0077 - acc: 0.9986 - ignore_accuracy: 0.9854 - val_loss: 0.0464 - val_acc: 0.9890 - val_ignore_accuracy: 0.8871\n",
            "Epoch 89/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0074 - acc: 0.9987 - ignore_accuracy: 0.9860 - val_loss: 0.0460 - val_acc: 0.9890 - val_ignore_accuracy: 0.8867\n",
            "Epoch 90/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0072 - acc: 0.9987 - ignore_accuracy: 0.9862 - val_loss: 0.0467 - val_acc: 0.9890 - val_ignore_accuracy: 0.8865\n",
            "Epoch 91/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0071 - acc: 0.9987 - ignore_accuracy: 0.9863 - val_loss: 0.0464 - val_acc: 0.9890 - val_ignore_accuracy: 0.8872\n",
            "Epoch 92/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0069 - acc: 0.9988 - ignore_accuracy: 0.9872 - val_loss: 0.0468 - val_acc: 0.9891 - val_ignore_accuracy: 0.8874\n",
            "Epoch 93/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0066 - acc: 0.9988 - ignore_accuracy: 0.9872 - val_loss: 0.0469 - val_acc: 0.9890 - val_ignore_accuracy: 0.8873\n",
            "Epoch 94/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0064 - acc: 0.9989 - ignore_accuracy: 0.9881 - val_loss: 0.0464 - val_acc: 0.9891 - val_ignore_accuracy: 0.8877\n",
            "Epoch 95/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0062 - acc: 0.9989 - ignore_accuracy: 0.9883 - val_loss: 0.0468 - val_acc: 0.9891 - val_ignore_accuracy: 0.8878\n",
            "Epoch 96/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0060 - acc: 0.9989 - ignore_accuracy: 0.9888 - val_loss: 0.0467 - val_acc: 0.9891 - val_ignore_accuracy: 0.8873\n",
            "Epoch 97/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0059 - acc: 0.9989 - ignore_accuracy: 0.9889 - val_loss: 0.0467 - val_acc: 0.9891 - val_ignore_accuracy: 0.8876\n",
            "Epoch 98/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0057 - acc: 0.9990 - ignore_accuracy: 0.9896 - val_loss: 0.0469 - val_acc: 0.9891 - val_ignore_accuracy: 0.8877\n",
            "Epoch 99/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0055 - acc: 0.9990 - ignore_accuracy: 0.9897 - val_loss: 0.0471 - val_acc: 0.9891 - val_ignore_accuracy: 0.8882\n",
            "Epoch 100/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0054 - acc: 0.9991 - ignore_accuracy: 0.9901 - val_loss: 0.0471 - val_acc: 0.9889 - val_ignore_accuracy: 0.8860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "fr0DO2e522wU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "89a285d6-ee69-4f71-a60a-b8c9ece298d9"
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_samples_X)\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['VBG', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "dWsUFxFb22wZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see the great difference in accuracy when we ignore the padding symbol. Also, note that the results reported by the original post https://nlpforhackers.io/lstm-pos-tagger-keras/ are better, probably because they used two LSTM chains: one going forwards, and another going backwards. This is what is called a **bidirectional recurrent network**, and it usually reports better results because each prediction is based on the context on the left and the right of the token."
      ]
    },
    {
      "metadata": {
        "id": "QFJouksxEsDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9da0b7d1-60ca-4ddf-d50f-95a29b09dfcb"
      },
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'val_ignore_accuracy', 'loss', 'acc', 'ignore_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "L_ieDH5d22wb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "e62eb444-1cca-4f59-9892-ecfeacecdd7a"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "ignore_acc = history.history['ignore_accuracy']\n",
        "val_ignore_acc = history.history['val_ignore_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.plot(epochs, ignore_acc, 'bo', label='Training ignore acc')\n",
        "plt.plot(epochs, val_ignore_acc, 'b', label='Validation ignore acc')\n",
        "plt.title('Training and validation ignore accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEHCAYAAACX/oD+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BPsYClKQFTtc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}