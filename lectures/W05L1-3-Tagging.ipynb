{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W05L1-3-Tagging.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "AWyCmzO622th",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building a Part of Speech Tagger with Keras\n",
        "\n",
        "This notebook is based on nlpforhackers' post: https://nlpforhackers.io/lstm-pos-tagger-keras/. We will look at how to build a part of speech tagger using a LSTM layer.\n",
        "\n",
        "We will use NLTK's treebank corpus. This corpus has, among other kinds of information, annotations about the parts of speech of the words."
      ]
    },
    {
      "metadata": {
        "id": "dugp1ZSX22tm",
        "colab_type": "code",
        "outputId": "9100f3ec-841b-4feb-cc25-fad4239b0478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        " \n",
        "print(tagged_sentences[0])\n",
        "print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "Tagged sentences:  3914\n",
            "Tagged words: 100676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "aKG3Wi1k22tx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As always, before training a model, we need to split the data in training and testing data:"
      ]
    },
    {
      "metadata": {
        "id": "B0BTKbIe22tz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(1234)\n",
        "tagged_sentences = list(tagged_sentences) # we convert the data to a list\n",
        "random.shuffle(tagged_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sEkj81QG22t4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "threshold = int(len(tagged_sentences)*.6)\n",
        "train = tagged_sentences[:threshold]\n",
        "test = tagged_sentences[threshold:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "42eTQ3kQ22t9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let’s restructure the data a bit. Let’s separate the words from the tags."
      ]
    },
    {
      "metadata": {
        "id": "J-DZLM5e22t_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        " \n",
        "train_sentences, train_tags =[], [] \n",
        "for tagged_sentence in train:\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    train_sentences.append(np.array(sentence))\n",
        "    train_tags.append(np.array(tags))\n",
        "    \n",
        "test_sentences, test_tags =[], [] \n",
        "for tagged_sentence in test:\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    test_sentences.append(np.array(sentence))\n",
        "    test_tags.append(np.array(tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aKlHJ21O22uE",
        "colab_type": "code",
        "outputId": "9ae85f10-7b45-4096-99cd-8ef06861ba83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_sentences[5])\n",
        "print(train_tags[5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['They' 'are' 'keeping' 'a' 'close' 'watch' 'on' 'the' 'yield' 'on' 'the'\n",
            " 'S&P' '500' '.']\n",
            "['PRP' 'VBP' 'VBG' 'DT' 'JJ' 'NN' 'IN' 'DT' 'NN' 'IN' 'DT' 'NNP' 'CD' '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "TO6Fo_ez22uK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now convert the words and tags to indices. We will reserve index 0 for padding, and index 1 for words that are out of the vocabulary (**OOV - Out of Vocabulary**)."
      ]
    },
    {
      "metadata": {
        "id": "Dpy45jUr22uM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words, tags = set([]), set([])\n",
        " \n",
        "for s in train_sentences:\n",
        "    for w in s:\n",
        "        words.add(w.lower())\n",
        " \n",
        "for ts in train_tags:\n",
        "    for t in ts:\n",
        "        tags.add(t)\n",
        " \n",
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # The special value used for padding\n",
        "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
        " \n",
        "tag2index = {t: i + 2 for i, t in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0  # The special value used to padding\n",
        "tag2index['-OOV-'] = 1  # There may also be unknown tags in the test data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "tz7jJt1R22uQ",
        "colab_type": "code",
        "outputId": "c1aef435-600b-4805-a949-9f5e72f1becc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        " \n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    s_int = []\n",
        "    for t in s:\n",
        "        try:\n",
        "            s_int.append(tag2index[t])\n",
        "        except KeyError:\n",
        "            s_int.append(tag2index['-OOV-'])\n",
        "            \n",
        "    test_tags_y.append(s_int)\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4303, 389, 7149, 3356, 4538, 1088, 6477, 6831]\n",
            "[4842, 5670, 2706, 5799, 3360, 1, 7364, 7238, 3651, 162, 4296, 7238, 1, 5767, 3180, 4538, 3741, 6555, 7238, 5256, 8507, 1, 6831]\n",
            "[7, 45, 29, 34, 38, 19, 19, 16]\n",
            "[33, 34, 30, 28, 34, 30, 14, 18, 10, 33, 30, 18, 30, 14, 30, 38, 20, 30, 18, 14, 43, 30, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "tyBmXkCw22uW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since Keras can only deal with fixed size sequences, we will pad all sequences to fit the longest sentence in the training data."
      ]
    },
    {
      "metadata": {
        "id": "e0ZdPDZA22uY",
        "colab_type": "code",
        "outputId": "735a8b93-98cd-40ff-cddb-7a0aa6f742c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(MAX_LENGTH)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "S8fOCFoD22ud",
        "colab_type": "code",
        "outputId": "c91f0a07-ec7a-4fb7-c4cb-6700d1f21aa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1187
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[4303  389 7149 3356 4538 1088 6477 6831    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[4842 5670 2706 5799 3360    1 7364 7238 3651  162 4296 7238    1 5767\n",
            " 3180 4538 3741 6555 7238 5256 8507    1 6831    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[ 7 45 29 34 38 19 19 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n",
            "[33 34 30 28 34 30 14 18 10 33 30 18 30 14 30 38 20 30 18 14 43 30 16  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "f0w8DzeQ22uj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now define the model. \n",
        "A simple approach to build a part-of-speech tagger in Keras is to stack a dense layer at the output of each RNN cell. This is achieved with `TimeDistributed`. Note  how the output of the model is now a list of sequences of vectors. Each vector will contain the probability of each tag. In particular:\n",
        "\n",
        "* We will introduce an embedding layer.\n",
        "* THe LSTM layer will have the parameter `return_sequences=True` so that we have access to the entire sequence.\n",
        "* The output of each LSTM cell will have a `Dense` layer. We can do this using the `TimeDistributed` layer."
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "f6buON2e22ul",
        "colab_type": "code",
        "outputId": "734aff66-2695-4a76-b2de-a7c5fa4a6040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 271, 128)          1111424   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 271, 256)          394240    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 271, 47)           12079     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 1,517,743\n",
            "Trainable params: 1,517,743\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "UeE6WfGH22ur",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We also need to convert the labels into one-hot encoding. Since the input are sequences of labels we cannot use Keras' `to_categorical` so we define our own function:"
      ]
    },
    {
      "metadata": {
        "id": "bJTyy7hA22uu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xWr3YL_S22uy",
        "colab_type": "code",
        "outputId": "78fd46b4-8ab1-4853-f8fd-9133c343c797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "cell_type": "code",
      "source": [
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "print(cat_train_tags_y[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "XLqIilKl22u5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can finally train the model:"
      ]
    },
    {
      "metadata": {
        "id": "7pTh_pR922vA",
        "colab_type": "code",
        "outputId": "ac27ebf5-9172-469c-9ff2-34817c3c0a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1583
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=40, validation_split=0.2)\n",
        " "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 1878 samples, validate on 470 samples\n",
            "Epoch 1/40\n",
            "1878/1878 [==============================] - 12s 6ms/step - loss: 1.7439 - acc: 0.8436 - val_loss: 0.6010 - val_acc: 0.9036\n",
            "Epoch 2/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.6316 - acc: 0.8845 - val_loss: 0.4542 - val_acc: 0.9037\n",
            "Epoch 3/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.4090 - acc: 0.9056 - val_loss: 0.3928 - val_acc: 0.9038\n",
            "Epoch 4/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3843 - acc: 0.9058 - val_loss: 0.3834 - val_acc: 0.9039\n",
            "Epoch 5/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3704 - acc: 0.9060 - val_loss: 0.3746 - val_acc: 0.9041\n",
            "Epoch 6/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3600 - acc: 0.9062 - val_loss: 0.3602 - val_acc: 0.9041\n",
            "Epoch 7/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3449 - acc: 0.9066 - val_loss: 0.3520 - val_acc: 0.9050\n",
            "Epoch 8/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3357 - acc: 0.9080 - val_loss: 0.3454 - val_acc: 0.9060\n",
            "Epoch 9/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3314 - acc: 0.9097 - val_loss: 0.3361 - val_acc: 0.9096\n",
            "Epoch 10/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3230 - acc: 0.9109 - val_loss: 0.3395 - val_acc: 0.9096\n",
            "Epoch 11/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3168 - acc: 0.9132 - val_loss: 0.3370 - val_acc: 0.9120\n",
            "Epoch 12/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3112 - acc: 0.9146 - val_loss: 0.3433 - val_acc: 0.9128\n",
            "Epoch 13/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3069 - acc: 0.9158 - val_loss: 0.3557 - val_acc: 0.9134\n",
            "Epoch 14/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3019 - acc: 0.9170 - val_loss: 0.3368 - val_acc: 0.9157\n",
            "Epoch 15/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2989 - acc: 0.9184 - val_loss: 0.3295 - val_acc: 0.9166\n",
            "Epoch 16/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2936 - acc: 0.9194 - val_loss: 0.3215 - val_acc: 0.9175\n",
            "Epoch 17/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2899 - acc: 0.9200 - val_loss: 0.3315 - val_acc: 0.9174\n",
            "Epoch 18/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2875 - acc: 0.9203 - val_loss: 0.3321 - val_acc: 0.9173\n",
            "Epoch 19/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2852 - acc: 0.9206 - val_loss: 0.3030 - val_acc: 0.9190\n",
            "Epoch 20/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2826 - acc: 0.9208 - val_loss: 0.2933 - val_acc: 0.9196\n",
            "Epoch 21/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2806 - acc: 0.9209 - val_loss: 0.2909 - val_acc: 0.9198\n",
            "Epoch 22/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2778 - acc: 0.9211 - val_loss: 0.2937 - val_acc: 0.9196\n",
            "Epoch 23/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2752 - acc: 0.9219 - val_loss: 0.2893 - val_acc: 0.9210\n",
            "Epoch 24/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2726 - acc: 0.9233 - val_loss: 0.2879 - val_acc: 0.9212\n",
            "Epoch 25/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2697 - acc: 0.9242 - val_loss: 0.2878 - val_acc: 0.9219\n",
            "Epoch 26/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2661 - acc: 0.9253 - val_loss: 0.2826 - val_acc: 0.9263\n",
            "Epoch 27/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2619 - acc: 0.9288 - val_loss: 0.2790 - val_acc: 0.9283\n",
            "Epoch 28/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2568 - acc: 0.9324 - val_loss: 0.2738 - val_acc: 0.9311\n",
            "Epoch 29/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2499 - acc: 0.9357 - val_loss: 0.2657 - val_acc: 0.9343\n",
            "Epoch 30/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2413 - acc: 0.9392 - val_loss: 0.2576 - val_acc: 0.9370\n",
            "Epoch 31/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2316 - acc: 0.9418 - val_loss: 0.2468 - val_acc: 0.9385\n",
            "Epoch 32/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2206 - acc: 0.9445 - val_loss: 0.2317 - val_acc: 0.9440\n",
            "Epoch 33/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2088 - acc: 0.9477 - val_loss: 0.2192 - val_acc: 0.9463\n",
            "Epoch 34/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1969 - acc: 0.9510 - val_loss: 0.2078 - val_acc: 0.9486\n",
            "Epoch 35/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1844 - acc: 0.9544 - val_loss: 0.1961 - val_acc: 0.9523\n",
            "Epoch 36/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1722 - acc: 0.9577 - val_loss: 0.1874 - val_acc: 0.9539\n",
            "Epoch 37/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1601 - acc: 0.9609 - val_loss: 0.1740 - val_acc: 0.9585\n",
            "Epoch 38/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1489 - acc: 0.9641 - val_loss: 0.1717 - val_acc: 0.9572\n",
            "Epoch 39/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1384 - acc: 0.9671 - val_loss: 0.1551 - val_acc: 0.9629\n",
            "Epoch 40/40\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1275 - acc: 0.9704 - val_loss: 0.1448 - val_acc: 0.9662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "5zNVFn7W22vM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the results look too good, it's because they are too good! We need to ignore the classification of the padded symbols, but let's leave it aside for now. Below is an evaluation using the test data."
      ]
    },
    {
      "metadata": {
        "id": "8YVnb9mv22vP",
        "colab_type": "code",
        "outputId": "68d0e3d2-6464-4f23-e4e8-7d171703a4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
        "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1566/1566 [==============================] - 11s 7ms/step\n",
            "acc: 96.64550643618841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "h197tLfl22vY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's find the PoS of two test sentences:"
      ]
    },
    {
      "metadata": {
        "id": "TJn9HsNN22va",
        "colab_type": "code",
        "outputId": "6d458fae-6abd-44bb-abbf-f00a034475c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "test_samples = [\n",
        "    \"running is very important for me .\".split(),\n",
        "    \"I was running every day for a month .\".split()\n",
        "]\n",
        "print(test_samples)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "NPD1nbGV22vf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These test sentences need to be vectorised:"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "DUSoAyOS22vg",
        "colab_type": "code",
        "outputId": "ed64225c-af56-4d16-e01d-887909971253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "cell_type": "code",
      "source": [
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "    test_samples_X.append(s_int)\n",
        " \n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "print(test_samples_X)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1947 1538  307 3351 7766 3060 6831    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [6467 2016 1947 1313 4424 7766 6040 2358 6831    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "Y6rLuQb322vl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And the predictions are ..."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "r5Qgjrq622vn",
        "colab_type": "code",
        "outputId": "0381640b-32d1-492a-ec60-e4f990315b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_samples_X)\n",
        "print(predictions, predictions.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[3.76531594e-02 1.47297652e-02 2.50692926e-02 ... 1.71638988e-02\n",
            "   3.13185081e-02 2.01013293e-02]\n",
            "  [3.76450792e-02 1.02725299e-02 2.28186492e-02 ... 1.47819789e-02\n",
            "   3.41623910e-02 2.07608156e-02]\n",
            "  [3.43763158e-02 5.01843169e-03 2.02158727e-02 ... 1.14209661e-02\n",
            "   4.11723331e-02 2.14138813e-02]\n",
            "  ...\n",
            "  [9.99945760e-01 2.47379238e-13 1.26558264e-10 ... 1.80510606e-11\n",
            "   9.91440288e-07 5.38341108e-11]\n",
            "  [9.99945760e-01 2.47376880e-13 1.26557778e-10 ... 1.80509583e-11\n",
            "   9.91438355e-07 5.38339026e-11]\n",
            "  [9.99945879e-01 2.47375037e-13 1.26557057e-10 ... 1.80507883e-11\n",
            "   9.91434717e-07 5.38336008e-11]]\n",
            "\n",
            " [[2.73831058e-02 1.55562880e-02 2.54227985e-02 ... 1.75938290e-02\n",
            "   2.91947685e-02 2.06104014e-02]\n",
            "  [3.70595083e-02 1.03689255e-02 2.79295594e-02 ... 1.45388581e-02\n",
            "   3.85050736e-02 1.98225062e-02]\n",
            "  [6.10151142e-02 4.14086645e-03 2.80714016e-02 ... 1.02367019e-02\n",
            "   5.41988276e-02 1.89223159e-02]\n",
            "  ...\n",
            "  [9.99945879e-01 2.47275019e-13 1.26526664e-10 ... 1.80434886e-11\n",
            "   9.91287266e-07 5.38207673e-11]\n",
            "  [9.99945879e-01 2.47274071e-13 1.26526428e-10 ... 1.80434209e-11\n",
            "   9.91286356e-07 5.38206667e-11]\n",
            "  [9.99945879e-01 2.47272201e-13 1.26525943e-10 ... 1.80433186e-11\n",
            "   9.91285447e-07 5.38204620e-11]]] (2, 271, 47)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "hYyJkHb_22vt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These predictions show, for each word, the probabilities assigned to each possible label. Let's choose the label with the highest probability (using numpy's `argmax`) and then convert from label index to label:"
      ]
    },
    {
      "metadata": {
        "id": "s1EEpd2K22vv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "P-vgNHpt22vz",
        "colab_type": "code",
        "outputId": "b0692b26-2085-4c1b-dc8c-4ff32593957a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['-PAD-', 'NNP', 'NNP', 'NN', 'IN', 'NNS', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['DT', 'DT', '-PAD-', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "xUAVFac_22v6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As expected, the padding symbol is over-used. To solve this problem we can define a custom accuracy metric that ignores the paddings. Remember that the index for the padding symbol is 0:"
      ]
    },
    {
      "metadata": {
        "id": "htLSFx1722v8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        " \n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "owboYAh022v_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now retrain using this new accuracy metric:"
      ]
    },
    {
      "metadata": {
        "id": "Adx9EbTB22wC",
        "colab_type": "code",
        "outputId": "d547c2d4-bf91-4c91-c219-5b229f31b067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 271, 128)          1111424   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 271, 256)          394240    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 271, 47)           12079     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 1,517,743\n",
            "Trainable params: 1,517,743\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "vglktjZL22wI",
        "colab_type": "code",
        "outputId": "da3f6795-596d-4522-b33f-924fb2a72728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3635
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "history = model.fit(train_sentences_X, cat_train_tags_y, batch_size=128, epochs=100, validation_split=0.2) #,\n",
        "#                    callbacks=[EarlyStopping(monitor='val_ignore_accuracy')])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1878 samples, validate on 470 samples\n",
            "Epoch 1/100\n",
            "1878/1878 [==============================] - 10s 5ms/step - loss: 1.8117 - acc: 0.8438 - ignore_accuracy: 0.0062 - val_loss: 0.6134 - val_acc: 0.9036 - val_ignore_accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.5924 - acc: 0.8922 - ignore_accuracy: 0.0467 - val_loss: 0.4488 - val_acc: 0.9039 - val_ignore_accuracy: 0.1309\n",
            "Epoch 3/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.4093 - acc: 0.9061 - ignore_accuracy: 0.1774 - val_loss: 0.4021 - val_acc: 0.9041 - val_ignore_accuracy: 0.1892\n",
            "Epoch 4/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3896 - acc: 0.9061 - ignore_accuracy: 0.2106 - val_loss: 0.3980 - val_acc: 0.9039 - val_ignore_accuracy: 0.2012\n",
            "Epoch 5/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3749 - acc: 0.9059 - ignore_accuracy: 0.1553 - val_loss: 0.3839 - val_acc: 0.9039 - val_ignore_accuracy: 0.1400\n",
            "Epoch 6/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3637 - acc: 0.9060 - ignore_accuracy: 0.1498 - val_loss: 0.3794 - val_acc: 0.9039 - val_ignore_accuracy: 0.0923\n",
            "Epoch 7/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3555 - acc: 0.9060 - ignore_accuracy: 0.1210 - val_loss: 0.3752 - val_acc: 0.9039 - val_ignore_accuracy: 0.0732\n",
            "Epoch 8/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3466 - acc: 0.9061 - ignore_accuracy: 0.1141 - val_loss: 0.3799 - val_acc: 0.9040 - val_ignore_accuracy: 0.0744\n",
            "Epoch 9/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3384 - acc: 0.9063 - ignore_accuracy: 0.1120 - val_loss: 0.3682 - val_acc: 0.9041 - val_ignore_accuracy: 0.0718\n",
            "Epoch 10/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3313 - acc: 0.9069 - ignore_accuracy: 0.1147 - val_loss: 0.3688 - val_acc: 0.9053 - val_ignore_accuracy: 0.1151\n",
            "Epoch 11/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3258 - acc: 0.9091 - ignore_accuracy: 0.1200 - val_loss: 0.3652 - val_acc: 0.9083 - val_ignore_accuracy: 0.1279\n",
            "Epoch 12/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3238 - acc: 0.9115 - ignore_accuracy: 0.1246 - val_loss: 0.3485 - val_acc: 0.9111 - val_ignore_accuracy: 0.1292\n",
            "Epoch 13/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3238 - acc: 0.9132 - ignore_accuracy: 0.1270 - val_loss: 0.3644 - val_acc: 0.9103 - val_ignore_accuracy: 0.1270\n",
            "Epoch 14/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3155 - acc: 0.9135 - ignore_accuracy: 0.1262 - val_loss: 0.3506 - val_acc: 0.9121 - val_ignore_accuracy: 0.1316\n",
            "Epoch 15/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3164 - acc: 0.9145 - ignore_accuracy: 0.1296 - val_loss: 0.3692 - val_acc: 0.9109 - val_ignore_accuracy: 0.1289\n",
            "Epoch 16/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3093 - acc: 0.9147 - ignore_accuracy: 0.1290 - val_loss: 0.3444 - val_acc: 0.9132 - val_ignore_accuracy: 0.1345\n",
            "Epoch 17/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3053 - acc: 0.9157 - ignore_accuracy: 0.1318 - val_loss: 0.3532 - val_acc: 0.9135 - val_ignore_accuracy: 0.1349\n",
            "Epoch 18/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.3013 - acc: 0.9162 - ignore_accuracy: 0.1328 - val_loss: 0.3613 - val_acc: 0.9140 - val_ignore_accuracy: 0.1392\n",
            "Epoch 19/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2976 - acc: 0.9170 - ignore_accuracy: 0.1378 - val_loss: 0.3683 - val_acc: 0.9146 - val_ignore_accuracy: 0.1436\n",
            "Epoch 20/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2944 - acc: 0.9181 - ignore_accuracy: 0.1472 - val_loss: 0.3776 - val_acc: 0.9150 - val_ignore_accuracy: 0.1481\n",
            "Epoch 21/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2917 - acc: 0.9194 - ignore_accuracy: 0.1584 - val_loss: 0.3835 - val_acc: 0.9154 - val_ignore_accuracy: 0.1525\n",
            "Epoch 22/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2899 - acc: 0.9200 - ignore_accuracy: 0.1644 - val_loss: 0.3197 - val_acc: 0.9141 - val_ignore_accuracy: 0.1457\n",
            "Epoch 23/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2953 - acc: 0.9197 - ignore_accuracy: 0.1653 - val_loss: 0.3090 - val_acc: 0.9194 - val_ignore_accuracy: 0.1759\n",
            "Epoch 24/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2864 - acc: 0.9207 - ignore_accuracy: 0.1711 - val_loss: 0.3081 - val_acc: 0.9202 - val_ignore_accuracy: 0.1857\n",
            "Epoch 25/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2826 - acc: 0.9224 - ignore_accuracy: 0.1876 - val_loss: 0.3077 - val_acc: 0.9212 - val_ignore_accuracy: 0.1960\n",
            "Epoch 26/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2794 - acc: 0.9233 - ignore_accuracy: 0.1956 - val_loss: 0.3163 - val_acc: 0.9211 - val_ignore_accuracy: 0.1984\n",
            "Epoch 27/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2764 - acc: 0.9234 - ignore_accuracy: 0.1962 - val_loss: 0.3137 - val_acc: 0.9212 - val_ignore_accuracy: 0.1985\n",
            "Epoch 28/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2734 - acc: 0.9236 - ignore_accuracy: 0.1970 - val_loss: 0.3248 - val_acc: 0.9211 - val_ignore_accuracy: 0.2025\n",
            "Epoch 29/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2703 - acc: 0.9245 - ignore_accuracy: 0.2067 - val_loss: 0.3140 - val_acc: 0.9220 - val_ignore_accuracy: 0.2087\n",
            "Epoch 30/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2667 - acc: 0.9254 - ignore_accuracy: 0.2152 - val_loss: 0.3111 - val_acc: 0.9230 - val_ignore_accuracy: 0.2199\n",
            "Epoch 31/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2627 - acc: 0.9271 - ignore_accuracy: 0.2332 - val_loss: 0.3102 - val_acc: 0.9244 - val_ignore_accuracy: 0.2374\n",
            "Epoch 32/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2578 - acc: 0.9287 - ignore_accuracy: 0.2504 - val_loss: 0.2986 - val_acc: 0.9271 - val_ignore_accuracy: 0.2647\n",
            "Epoch 33/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2521 - acc: 0.9317 - ignore_accuracy: 0.2826 - val_loss: 0.2955 - val_acc: 0.9270 - val_ignore_accuracy: 0.2656\n",
            "Epoch 34/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2452 - acc: 0.9335 - ignore_accuracy: 0.3014 - val_loss: 0.2799 - val_acc: 0.9303 - val_ignore_accuracy: 0.2976\n",
            "Epoch 35/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2371 - acc: 0.9356 - ignore_accuracy: 0.3246 - val_loss: 0.2693 - val_acc: 0.9321 - val_ignore_accuracy: 0.3163\n",
            "Epoch 36/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2281 - acc: 0.9378 - ignore_accuracy: 0.3482 - val_loss: 0.2640 - val_acc: 0.9339 - val_ignore_accuracy: 0.3396\n",
            "Epoch 37/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2180 - acc: 0.9410 - ignore_accuracy: 0.3826 - val_loss: 0.2445 - val_acc: 0.9380 - val_ignore_accuracy: 0.3797\n",
            "Epoch 38/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.2070 - acc: 0.9449 - ignore_accuracy: 0.4251 - val_loss: 0.2245 - val_acc: 0.9440 - val_ignore_accuracy: 0.4365\n",
            "Epoch 39/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1958 - acc: 0.9492 - ignore_accuracy: 0.4708 - val_loss: 0.2158 - val_acc: 0.9463 - val_ignore_accuracy: 0.4652\n",
            "Epoch 40/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1841 - acc: 0.9533 - ignore_accuracy: 0.5144 - val_loss: 0.2048 - val_acc: 0.9491 - val_ignore_accuracy: 0.4949\n",
            "Epoch 41/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.5448 - acc: 0.8959 - ignore_accuracy: 0.5117 - val_loss: 0.2227 - val_acc: 0.9511 - val_ignore_accuracy: 0.5035\n",
            "Epoch 42/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1996 - acc: 0.9502 - ignore_accuracy: 0.4968 - val_loss: 0.2030 - val_acc: 0.9510 - val_ignore_accuracy: 0.5072\n",
            "Epoch 43/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1835 - acc: 0.9537 - ignore_accuracy: 0.5357 - val_loss: 0.1876 - val_acc: 0.9521 - val_ignore_accuracy: 0.5380\n",
            "Epoch 44/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1706 - acc: 0.9581 - ignore_accuracy: 0.5796 - val_loss: 0.1774 - val_acc: 0.9572 - val_ignore_accuracy: 0.5750\n",
            "Epoch 45/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1617 - acc: 0.9611 - ignore_accuracy: 0.6066 - val_loss: 0.1701 - val_acc: 0.9597 - val_ignore_accuracy: 0.5996\n",
            "Epoch 46/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1543 - acc: 0.9632 - ignore_accuracy: 0.6270 - val_loss: 0.1638 - val_acc: 0.9605 - val_ignore_accuracy: 0.6074\n",
            "Epoch 47/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1477 - acc: 0.9647 - ignore_accuracy: 0.6411 - val_loss: 0.1582 - val_acc: 0.9618 - val_ignore_accuracy: 0.6199\n",
            "Epoch 48/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1416 - acc: 0.9661 - ignore_accuracy: 0.6550 - val_loss: 0.1528 - val_acc: 0.9629 - val_ignore_accuracy: 0.6302\n",
            "Epoch 49/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1358 - acc: 0.9675 - ignore_accuracy: 0.6690 - val_loss: 0.1477 - val_acc: 0.9641 - val_ignore_accuracy: 0.6421\n",
            "Epoch 50/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1302 - acc: 0.9691 - ignore_accuracy: 0.6847 - val_loss: 0.1429 - val_acc: 0.9650 - val_ignore_accuracy: 0.6510\n",
            "Epoch 51/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1250 - acc: 0.9704 - ignore_accuracy: 0.6978 - val_loss: 0.1383 - val_acc: 0.9661 - val_ignore_accuracy: 0.6600\n",
            "Epoch 52/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1199 - acc: 0.9719 - ignore_accuracy: 0.7121 - val_loss: 0.1338 - val_acc: 0.9677 - val_ignore_accuracy: 0.6757\n",
            "Epoch 53/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1150 - acc: 0.9732 - ignore_accuracy: 0.7257 - val_loss: 0.1296 - val_acc: 0.9689 - val_ignore_accuracy: 0.6872\n",
            "Epoch 54/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1103 - acc: 0.9747 - ignore_accuracy: 0.7407 - val_loss: 0.1256 - val_acc: 0.9703 - val_ignore_accuracy: 0.7009\n",
            "Epoch 55/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1058 - acc: 0.9759 - ignore_accuracy: 0.7535 - val_loss: 0.1216 - val_acc: 0.9715 - val_ignore_accuracy: 0.7126\n",
            "Epoch 56/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.1013 - acc: 0.9775 - ignore_accuracy: 0.7694 - val_loss: 0.1178 - val_acc: 0.9728 - val_ignore_accuracy: 0.7249\n",
            "Epoch 57/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0970 - acc: 0.9788 - ignore_accuracy: 0.7821 - val_loss: 0.1142 - val_acc: 0.9734 - val_ignore_accuracy: 0.7314\n",
            "Epoch 58/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0929 - acc: 0.9799 - ignore_accuracy: 0.7928 - val_loss: 0.1106 - val_acc: 0.9746 - val_ignore_accuracy: 0.7422\n",
            "Epoch 59/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0888 - acc: 0.9809 - ignore_accuracy: 0.8033 - val_loss: 0.1072 - val_acc: 0.9754 - val_ignore_accuracy: 0.7502\n",
            "Epoch 60/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0849 - acc: 0.9819 - ignore_accuracy: 0.8131 - val_loss: 0.1038 - val_acc: 0.9763 - val_ignore_accuracy: 0.7598\n",
            "Epoch 61/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0810 - acc: 0.9829 - ignore_accuracy: 0.8233 - val_loss: 0.1006 - val_acc: 0.9770 - val_ignore_accuracy: 0.7669\n",
            "Epoch 62/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0774 - acc: 0.9837 - ignore_accuracy: 0.8318 - val_loss: 0.0975 - val_acc: 0.9777 - val_ignore_accuracy: 0.7728\n",
            "Epoch 63/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0738 - acc: 0.9846 - ignore_accuracy: 0.8404 - val_loss: 0.0945 - val_acc: 0.9783 - val_ignore_accuracy: 0.7800\n",
            "Epoch 64/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0704 - acc: 0.9854 - ignore_accuracy: 0.8487 - val_loss: 0.0917 - val_acc: 0.9791 - val_ignore_accuracy: 0.7874\n",
            "Epoch 65/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0671 - acc: 0.9863 - ignore_accuracy: 0.8583 - val_loss: 0.0891 - val_acc: 0.9799 - val_ignore_accuracy: 0.7949\n",
            "Epoch 66/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0640 - acc: 0.9871 - ignore_accuracy: 0.8671 - val_loss: 0.0866 - val_acc: 0.9804 - val_ignore_accuracy: 0.8004\n",
            "Epoch 67/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0610 - acc: 0.9879 - ignore_accuracy: 0.8752 - val_loss: 0.0840 - val_acc: 0.9810 - val_ignore_accuracy: 0.8058\n",
            "Epoch 68/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0581 - acc: 0.9886 - ignore_accuracy: 0.8817 - val_loss: 0.0817 - val_acc: 0.9815 - val_ignore_accuracy: 0.8106\n",
            "Epoch 69/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0553 - acc: 0.9892 - ignore_accuracy: 0.8885 - val_loss: 0.0795 - val_acc: 0.9822 - val_ignore_accuracy: 0.8180\n",
            "Epoch 70/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0527 - acc: 0.9899 - ignore_accuracy: 0.8955 - val_loss: 0.0776 - val_acc: 0.9827 - val_ignore_accuracy: 0.8230\n",
            "Epoch 71/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0502 - acc: 0.9905 - ignore_accuracy: 0.9016 - val_loss: 0.0755 - val_acc: 0.9832 - val_ignore_accuracy: 0.8282\n",
            "Epoch 72/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0478 - acc: 0.9909 - ignore_accuracy: 0.9057 - val_loss: 0.0737 - val_acc: 0.9835 - val_ignore_accuracy: 0.8312\n",
            "Epoch 73/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0456 - acc: 0.9913 - ignore_accuracy: 0.9101 - val_loss: 0.0721 - val_acc: 0.9840 - val_ignore_accuracy: 0.8358\n",
            "Epoch 74/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0435 - acc: 0.9919 - ignore_accuracy: 0.9157 - val_loss: 0.0707 - val_acc: 0.9844 - val_ignore_accuracy: 0.8401\n",
            "Epoch 75/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0415 - acc: 0.9922 - ignore_accuracy: 0.9194 - val_loss: 0.0690 - val_acc: 0.9846 - val_ignore_accuracy: 0.8423\n",
            "Epoch 76/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0397 - acc: 0.9926 - ignore_accuracy: 0.9228 - val_loss: 0.0676 - val_acc: 0.9848 - val_ignore_accuracy: 0.8450\n",
            "Epoch 77/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0379 - acc: 0.9930 - ignore_accuracy: 0.9267 - val_loss: 0.0663 - val_acc: 0.9851 - val_ignore_accuracy: 0.8475\n",
            "Epoch 78/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0362 - acc: 0.9933 - ignore_accuracy: 0.9298 - val_loss: 0.0650 - val_acc: 0.9852 - val_ignore_accuracy: 0.8489\n",
            "Epoch 79/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0347 - acc: 0.9936 - ignore_accuracy: 0.9328 - val_loss: 0.0640 - val_acc: 0.9855 - val_ignore_accuracy: 0.8515\n",
            "Epoch 80/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0332 - acc: 0.9938 - ignore_accuracy: 0.9356 - val_loss: 0.0631 - val_acc: 0.9856 - val_ignore_accuracy: 0.8531\n",
            "Epoch 81/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0318 - acc: 0.9940 - ignore_accuracy: 0.9378 - val_loss: 0.0621 - val_acc: 0.9858 - val_ignore_accuracy: 0.8545\n",
            "Epoch 82/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0305 - acc: 0.9944 - ignore_accuracy: 0.9408 - val_loss: 0.0611 - val_acc: 0.9859 - val_ignore_accuracy: 0.8557\n",
            "Epoch 83/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0294 - acc: 0.9945 - ignore_accuracy: 0.9426 - val_loss: 0.0602 - val_acc: 0.9861 - val_ignore_accuracy: 0.8577\n",
            "Epoch 84/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0282 - acc: 0.9947 - ignore_accuracy: 0.9444 - val_loss: 0.0594 - val_acc: 0.9863 - val_ignore_accuracy: 0.8596\n",
            "Epoch 85/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0271 - acc: 0.9949 - ignore_accuracy: 0.9467 - val_loss: 0.0588 - val_acc: 0.9864 - val_ignore_accuracy: 0.8612\n",
            "Epoch 86/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0261 - acc: 0.9951 - ignore_accuracy: 0.9485 - val_loss: 0.0583 - val_acc: 0.9865 - val_ignore_accuracy: 0.8613\n",
            "Epoch 87/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0251 - acc: 0.9953 - ignore_accuracy: 0.9505 - val_loss: 0.0575 - val_acc: 0.9867 - val_ignore_accuracy: 0.8633\n",
            "Epoch 88/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0242 - acc: 0.9954 - ignore_accuracy: 0.9520 - val_loss: 0.0572 - val_acc: 0.9869 - val_ignore_accuracy: 0.8660\n",
            "Epoch 89/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0234 - acc: 0.9957 - ignore_accuracy: 0.9548 - val_loss: 0.0565 - val_acc: 0.9870 - val_ignore_accuracy: 0.8665\n",
            "Epoch 90/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0226 - acc: 0.9959 - ignore_accuracy: 0.9569 - val_loss: 0.0561 - val_acc: 0.9871 - val_ignore_accuracy: 0.8669\n",
            "Epoch 91/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0218 - acc: 0.9960 - ignore_accuracy: 0.9577 - val_loss: 0.0556 - val_acc: 0.9872 - val_ignore_accuracy: 0.8682\n",
            "Epoch 92/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0211 - acc: 0.9961 - ignore_accuracy: 0.9591 - val_loss: 0.0551 - val_acc: 0.9872 - val_ignore_accuracy: 0.8689\n",
            "Epoch 93/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0204 - acc: 0.9962 - ignore_accuracy: 0.9603 - val_loss: 0.0546 - val_acc: 0.9872 - val_ignore_accuracy: 0.8687\n",
            "Epoch 94/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0198 - acc: 0.9963 - ignore_accuracy: 0.9615 - val_loss: 0.0544 - val_acc: 0.9874 - val_ignore_accuracy: 0.8704\n",
            "Epoch 95/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0192 - acc: 0.9964 - ignore_accuracy: 0.9623 - val_loss: 0.0542 - val_acc: 0.9873 - val_ignore_accuracy: 0.8696\n",
            "Epoch 96/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0186 - acc: 0.9965 - ignore_accuracy: 0.9632 - val_loss: 0.0539 - val_acc: 0.9874 - val_ignore_accuracy: 0.8709\n",
            "Epoch 97/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0180 - acc: 0.9966 - ignore_accuracy: 0.9641 - val_loss: 0.0535 - val_acc: 0.9875 - val_ignore_accuracy: 0.8709\n",
            "Epoch 98/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0175 - acc: 0.9967 - ignore_accuracy: 0.9649 - val_loss: 0.0534 - val_acc: 0.9875 - val_ignore_accuracy: 0.8709\n",
            "Epoch 99/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0170 - acc: 0.9968 - ignore_accuracy: 0.9658 - val_loss: 0.0529 - val_acc: 0.9875 - val_ignore_accuracy: 0.8710\n",
            "Epoch 100/100\n",
            "1878/1878 [==============================] - 9s 5ms/step - loss: 0.0166 - acc: 0.9969 - ignore_accuracy: 0.9667 - val_loss: 0.0534 - val_acc: 0.9876 - val_ignore_accuracy: 0.8722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "fr0DO2e522wU",
        "colab_type": "code",
        "outputId": "ad5adc48-7c6d-4222-ccec-a38dccaf7231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_samples_X)\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['MD', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "dWsUFxFb22wZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see the great difference in accuracy when we ignore the padding symbol. Also, note that the results reported by the original post https://nlpforhackers.io/lstm-pos-tagger-keras/ are better, probably because they used two LSTM chains: one going forwards, and another going backwards. This is what is called a **bidirectional recurrent network**, and it usually reports better results because each prediction is based on the context on the left and the right of the token."
      ]
    },
    {
      "metadata": {
        "id": "QFJouksxEsDM",
        "colab_type": "code",
        "outputId": "942bacca-6055-4030-8d8e-bca6f3882c2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'val_ignore_accuracy', 'loss', 'acc', 'ignore_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "L_ieDH5d22wb",
        "colab_type": "code",
        "outputId": "425a10e3-2770-44aa-90bf-7aaea1f351fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [15, 5]\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "ignore_acc = history.history['ignore_accuracy']\n",
        "val_ignore_acc = history.history['val_ignore_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.plot(epochs, ignore_acc, 'go', label='Training ignore acc')\n",
        "plt.plot(epochs, val_ignore_acc, 'g', label='Validation ignore acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAE+CAYAAAD4XjP+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI+E+OqYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BPsYClKQFTtc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}